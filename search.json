[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "",
    "text": "Welcome to the 2023 GEDI / ICESAT-2 Application Workshop hosted by the Space and Sustainability Colloquium with support from NASA Openscapes.\nThe workshop will take place in-person on November 16 2023 from 3:00pm-6:00pm CT (UTC -6).\nGET STARTED:\n\nDeploy Jupyter Lab instance in 2i2c\n\nWorkshop Schedule\nAccess notebooks without cloning"
  },
  {
    "objectID": "index.html#welcome-bienvenidos",
    "href": "index.html#welcome-bienvenidos",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "",
    "text": "Welcome to the 2023 GEDI / ICESAT-2 Application Workshop hosted by the Space and Sustainability Colloquium with support from NASA Openscapes.\nThe workshop will take place in-person on November 16 2023 from 3:00pm-6:00pm CT (UTC -6).\nGET STARTED:\n\nDeploy Jupyter Lab instance in 2i2c\n\nWorkshop Schedule\nAccess notebooks without cloning"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Description\nThe workshop will demonstrate how to find, access, and work with GEDI and Icesat-2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA’s Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2023 GEDI/ICESAT-2 Workshop is hosted by the Space and Sustainability Colloquium with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/\n\ntotal 128\ndrwxr-xr-x 25 jovyan jovyan  6144 Nov 11 18:59  .\ndrwxr-xr-x  1 root   root      20 Mar 11  2022  ..\n-rw-------  1 jovyan jovyan 12347 Nov 11 20:21  .bash_history\ndrwxr-xr-x  8 jovyan jovyan  6144 Nov  8 20:16  .cache\ndrwxrwsr-x  2 jovyan jovyan  6144 Apr 12  2022  .conda\ndrwxr-xr-x  4 jovyan jovyan  6144 Feb 28  2022  .config\ndrwx------  2 jovyan jovyan  6144 Nov 11 17:36  .git-credential-cache\n-rw-r--r--  1 jovyan jovyan    56 Nov  4 15:14  .gitconfig\ndrwxr-xr-x  3 jovyan jovyan  6144 Apr 13  2022  .hidden_dir\ndrwxr-xr-x  2 jovyan jovyan  6144 Nov 11 02:45  .ipynb_checkpoints\ndrwxr-xr-x  5 jovyan jovyan  6144 Jul 20  2021  .ipython\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .jupyter\n-rw-r--r--  1 jovyan jovyan     0 Jul 20  2021  .jupyter-server-log.txt\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .local\n-rw-------  1 jovyan jovyan    73 Nov 11 18:24  .netrc\ndrwx------  2 jovyan jovyan  6144 Aug  4 15:42  .ssh\ndrwxr-xr-x 12 jovyan jovyan  6144 Nov 11 05:57  2022-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 14 05:11  2022-Fall-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 15 jovyan jovyan  6144 Nov  9 19:18  2022-Fall-ECOSTRESS-Cloud-Workshop_MJ\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 11 16:43  2022-Fall-ECOSTRESS-Cloud-Workshop_mmm\ndrwxr-xr-x  5 jovyan jovyan  6144 Nov 10 14:37 'Untitled Folder'\ndrwxr-xr-x  5 jovyan jovyan  6144 Aug 29 15:17  appeears-cloud-optimized-format-prototype\n-rw-r--r--  1 jovyan jovyan   131 Nov 11 18:59  cookies.txt\ndrwxr-xr-x 15 jovyan jovyan  6144 Mar 10  2022  earthdata-cloud-cookbook\ndrwxr-xr-x  6 jovyan jovyan  6144 Apr 28  2022  lpdaac_cloud_data_access\ndrwxr-xr-x  6 jovyan jovyan  6144 Oct 20  2021  lpdaac_cloud_data_access1\ndrwxr-xr-x  4 jovyan jovyan  6144 Jul 20  2021  lpdaac_hls_tutorial\ndrwxr-xr-x  4 jovyan jovyan  6144 Sep 19 21:29  mentors-2022\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared-readwrite"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html",
    "title": "Introduction to xarray… and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "title": "Introduction to xarray… and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#what-is-xarray",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#what-is-xarray",
    "title": "Introduction to xarray… and hvplot",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "Introduction to xarray… and hvplot",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "title": "Introduction to xarray… and hvplot",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#analysis",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#analysis",
    "title": "Introduction to xarray… and hvplot",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#plot-results",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#plot-results",
    "title": "Introduction to xarray… and hvplot",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html",
    "href": "tutorials/data-access/earthaccess.html",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "Outline: * ICESat-2 Data Access, NASA DAAC @ NSIDC * Overview of ICESat-2 Mission and land products see USFS Application Workshop slides * NSIDC DAAC resources / tools * Walk through myriad options with different capabilities/use cases * Modified version of table from IS2 Hackweek * Basic earthaccess demo of ATL08 access * Need common area / time of interest across IS2 / GEDI tutorials\n\nThe icepyx Software Library and Community"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#tutorial-overview",
    "href": "tutorials/data-access/earthaccess.html#tutorial-overview",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "Outline: * ICESat-2 Data Access, NASA DAAC @ NSIDC * Overview of ICESat-2 Mission and land products see USFS Application Workshop slides * NSIDC DAAC resources / tools * Walk through myriad options with different capabilities/use cases * Modified version of table from IS2 Hackweek * Basic earthaccess demo of ATL08 access * Need common area / time of interest across IS2 / GEDI tutorials\n\nThe icepyx Software Library and Community"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#intro",
    "href": "tutorials/data-access/earthaccess.html#intro",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Intro",
    "text": "Intro\nThis notebook demonstrates searching for cloud-hosted ICESat-2 data and directly accessing ATL08 (Land and Vegetation Height) files (granules) from an Amazon Compute Cloud (EC2) instance using the earthaccess package. NASA data “in the cloud” are stored in Amazon Web Services (AWS) Simple Storage Service (S3) Buckets. Direct Access is an efficient way to work with data stored in an S3 Bucket when you are working in the cloud. Cloud-hosted granules can be opened and loaded into memory without the need to download them first. This allows you take advantage of the scalability and power of cloud computing.\nThe Amazon Global cloud is divided into geographical regions. To have direct access to data stored in a region, our compute instance - a virtual computer that we create to perform processing operations in place of using our own desktop or laptop - must be in the same region as the data. This is a fundamental concept of analysis in place. NASA cloud-hosted data is in Amazon Region us-west2. So your compute instance must also be in us-west2. If we wanted to use data stored in another region, to use direct access for that data, we would start a compute instance in that region.\nWe demonstrate how to open an HDF5 granule and access data variables using xarray. Land Ice Heights are then plotted using hvplot.\nearthaccess is a package that allows easy search of the NASA Common Metadata Repository (CMR) and download of NASA data collections. It can be used for programmatic search and access for both DAAC-hosted and cloud-hosted data. It manages authenticating using Earthdata Login credentials which are then used to obtain the S3 tokens that are needed for S3 direct access. https://github.com/nsidc/earthaccess"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#data-access-tools",
    "href": "tutorials/data-access/earthaccess.html#data-access-tools",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Data Access Tools",
    "text": "Data Access Tools"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#icesat-2-mission-data",
    "href": "tutorials/data-access/earthaccess.html#icesat-2-mission-data",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "ICESat-2 Mission Data",
    "text": "ICESat-2 Mission Data\n\n\n\nThis table is taken from Andy Barret’s tutorial at the ICESat-2 Hackweek on the different ways of accessing data from NASA Earth.\n\n\n\n\nicepyx\nearthaccess\nSliderule\nOpenAltimetry\nNASA Earthdata Search\nNSIDC data product pages\n\n\n\n\nFilter Spatially using:\n\n\n\n\n\n\n\n\n&gt; Interactive map widget\n\nx\nx\nx\nx\nx\n\n\n&gt; Bounding Box\nx\nx\nx\nx\nx\nx\n\n\n&gt; Polygon\nx\nx\nx\n\nx\nx\n\n\n&gt; GeoJSON or Shapefile\nx\n\nx\n\nx\nx\n\n\nFilter by time and date\nx\nx\nx\nx\nx\nx\n\n\nPreview data\nx\nx\n\nx\nx\nx\n\n\nDownload data from DAAC\nx\nx\n\nx\nx\nx\n\n\nAccess cloud-hosted data\nx\nx\nx\n\nx\n\n\n\nAll ICESat-2 data\nx\nx\n\n\nx\nx\n\n\nSubset (spatially, temporally, by variable)\nx\n\nx\nx\nx\n\n\n\nLoad data by direct-access\nx\nx\nx\n\n\n\n\n\nProcess and analyze data\n\n\nx\n\n\n\n\n\nPlot data with built-in methods\nx\n\nx\nx\n\n\n\n\n\n\nLearning Objectives\nBy the end of this demonstration you will be able to:\n1. use earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results;\n2. open data granules using direct access to the ICESat-2 S3 bucket;\n3. load a HDF5 group into an xarray.Dataset;\n4. visualize the land ice heights using hvplot.\n\n\nPrerequisites\nSee Prerequisites"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#tutorial-steps",
    "href": "tutorials/data-access/earthaccess.html#tutorial-steps",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "2. Tutorial steps",
    "text": "2. Tutorial steps"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#import-packages",
    "href": "tutorials/data-access/earthaccess.html#import-packages",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Import Packages",
    "text": "Import Packages\nThe first step in any python script or notebook is to import packages. This tutorial requires the following packages: - earthaccess, which enables Earthdata Login authentication and retrieves AWS credentials; enables collection and granule searches; and S3 access; - xarray, used to load data; - ipyleaflet, used for interactive maps in Jupyter - shapely, used to work with geometric objects - hvplot, used to visualize land ice height data.\nWe are going to import the whole earthaccess package.\nWe will also import the whole xarray package but use a standard short name xr, using the import &lt;package&gt; as &lt;short_name&gt; syntax. We could use anything for a short name but xr is an accepted standard that most xarray users are familiar with.\nWe only need the xarray module from hvplot so we import that using the import &lt;package&gt;.&lt;module&gt; syntax.\nSince we are going to use a developer version of earthaccess, we are going to install it directly from Github, after executing the next cell we need to restart the kernel.\n\n%%capture\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\nNote: we need to restart our Jupyter kernel\n\n# For searching NASA data\nimport earthaccess\nimport shapely\nimport json\nimport ipyleaflet\n\n# For reading data, analysis and plotting\nimport xarray as xr\nimport hvplot.xarray\nimport pprint"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#authenticate",
    "href": "tutorials/data-access/earthaccess.html#authenticate",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Authenticate",
    "text": "Authenticate\nThe first step is to get the correct authentication that will allow us to get cloud-hosted ICESat-2 data. This is all done through Earthdata Login. The login method also gets the correct AWS credentials.\nLogin requires your Earthdata Login username and password. The login method will automatically search for these credentials as environment variables or in a .netrc file, and if those aren’t available it will prompt us to enter our username and password. We use a .netrc strategy. A .netrc file is a text file located in our home directory that contains login information for remote machines. If we don’t have a .netrc file, login can create one for us.\nearthaccess.login(strategy='interactive', persist=True)\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#search-for-icesat-2-collections",
    "href": "tutorials/data-access/earthaccess.html#search-for-icesat-2-collections",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search for ICESat-2 Collections",
    "text": "Search for ICESat-2 Collections\nearthaccess leverages the Common Metadata Repository (CMR) API to search for collections and granules. Earthdata Search also uses the CMR API.\nWe can use the search_datasets method to search for ICESat-2 collections by setting keyword='ICESat-2'.\nThis will display the number of data collections (data sets) that meet this search criteria.\n\ndatasets = earthaccess.search_datasets(keyword = 'ICESat-2')\n\nIn this case there are 65 collections that have the keyword ICESat-2.\nThe search_datasets method returns a python list of DataCollection objects. We can view the metadata for each collection in long form by passing a DataCollection object to print or as a summary using the summary method. We can also use the pprint function to Pretty Print each object.\nWe will do this for the first 10 results (objects).\n\nfor collection in datasets[:4]:\n    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n    print('')\n\nFor each collection, summary returns a subset of fields from the collection metadata and the Unified Metadata Model (UMM): - concept-id is a unique id for the collection. It consists of an alphanumeric code and the provider-id specific to the DAAC (Distributed Active Archive Center). You can use the concept_id to search for data granules. - short_name is a quick way of referring to a collection (instead of using the full title). It can be found on the collection landing page underneath the collection title after ‘DATA SET ID’. See the table below for a list of the shortnames for ICESat-2 collections. - version is the version of each collection. - file-type gives information about the file format of the collection granules. - get-data is a collection of URLs that can be used to access the data, collection landing pages and data tools. - cloud-info this is for cloud-hosted data and provides additional information about the location of the S3 bucket that holds the data and where to get temporary AWS S3 credentials to access the S3 buckets. earthaccess handles these credentials and the links to the S3 buckets, so in general you won’t need to worry about this information.\nFor the ICESat-2 search results, within the concept-id, there is a provider-id; NSIDC_ECS and NSIDC_CPRD. NSIDC_ECS which is for the on-prem collections and NSIDC_CPRD is for the cloud-hosted collections.\n\nSearch for cloud-hosted data\nFor most collections, to search for only data in the cloud, the cloud_hosted method can be used.\n\ndatasets = earthaccess.search_datasets(\n    keyword = 'ICESat-2',\n    cloud_hosted = True\n)"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#search-a-data-set-using-spatial-and-temporal-filters",
    "href": "tutorials/data-access/earthaccess.html#search-a-data-set-using-spatial-and-temporal-filters",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search a data set using spatial and temporal filters",
    "text": "Search a data set using spatial and temporal filters\nWe can use the search_data method to search for granules within a data set by location and time using spatial and temporal filters. In this example, we will search for data granules from the following datasets:\n\nATLAS/ICESat-2 L3A Land and Vegetation Height, Version 6: C2596864127-NSIDC_CPRD\nGEDI L4A Footprint Level Aboveground Biomass Density, Version 2.1: C2237824918-ORNL_CLOUD\nGEDI L2A Elevation and Height Metrics Data Global Footprint Level V002: C1908348134-LPDAAC_ECS\n\nThe temporal range is identified with standard date strings, and latitude-longitude corners of a bounding box is specified. Polygons and points, as well as shapefiles can also be specified.\nThis will display the number of granules that match our search.\n\nwith open(\"bosque_primavera.json\") as f:\n    geojson = json.load(f)\n\ngeojosn_geom = geojson['features'][0]['geometry']\n# polygon = shapely.geometry.box(*bbox, ccw=True)\npolygon = shapely.geometry.shape(geojosn_geom)\npolygon\n\nNo we simplify the geometry of our polygon\n\nlen(polygon.boundary.coords)\n\n\np = polygon.simplify(0.01)\np\n\n\nlen(p.boundary.coords)\n\n\npolygon_coords = [(coord[0], coord[1]) for coord in p.boundary.coords]\n\n\nif \"map_widget\" in vars():\n    if len(map_widget.roi) &gt; 0:\n        map_widget = sw.roi\n\nparams = {\n    \"concept_id\" : [\"C2613553260-NSIDC_CPRD\", \"C1908348134-LPDAAC_ECS\", \"C2237824918-ORNL_CLOUD\"],\n    \"temporal\": (\"2022-08\", \"2023-09\"),\n    \"polygon\": polygon_coords\n}\n\nresults = earthaccess.search_data(**params)\n\nTo display the rendered metadata, including the download link, granule size and two images, we will use display. In the example below, all 4 results are shown.\nThe download link is https and can be used download the granule to your local machine. This is similar to downloading DAAC-hosted data but in this case the data are coming from the Earthdata Cloud. For NASA data in the Earthdata Cloud, there is no charge to the user for egress from AWS Cloud servers. This is not the case for other data in the cloud.\nNote the [None, None, None, None] that is displayed at the end can be ignored, it has no meaning in relation to the metadata.\n\npreviews = [display(r) for r in results[0:3]]"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#interactive-map-widget",
    "href": "tutorials/data-access/earthaccess.html#interactive-map-widget",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Interactive Map Widget",
    "text": "Interactive Map Widget\nAn upcoming feature of earthaccess is the map widget that will allow us to use custom maps for data selection and the exploration or results from NASA’s CMR very similar to what geopandas does with their .explore() method.\n\nmap_widget = earthaccess.search_widget()\nmap_widget.m.add(ipyleaflet.GeoJSON(name=\"ROI\",\n        data=geojosn_geom,\n        style={\n          \"color\": \"red\",\n          \"opacity\": 0.9,\n          \"fillOpacity\": 0.1\n          }\n        ))\nmap_widget.explore(results)"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#use-direct-access-to-open-load-and-display-data-stored-on-s3",
    "href": "tutorials/data-access/earthaccess.html#use-direct-access-to-open-load-and-display-data-stored-on-s3",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Use Direct-Access to open, load and display data stored on S3",
    "text": "Use Direct-Access to open, load and display data stored on S3\nDirect-access to data from an S3 bucket is a two step process. First, the files are opened using the open method. The auth object created at the start of the notebook is used to provide Earthdata Login authentication and AWS credentials.\nThe next step is to load the data. In this case, data are loaded into an xarray.Dataset. Data could be read into numpy arrays or a pandas.Dataframe. However, each granule would have to be read using a package that reads HDF5 granules such as h5py. xarray does this all under-the-hood in a single line but for a single group in the HDF5 granule*.\n*ICESat-2 measures photon returns from 3 beam pairs numbered 1, 2 and 3 that each consist of a left and a right beam. In this case, we are interested in the left ground track (gt) of beam pair 1.\n\nparams = {\n    \"concept_id\" : [\"C2613553260-NSIDC_CPRD\"],\n    \"temporal\": (\"2022-08\", \"2023-09\"),\n    \"polygon\": polygon_coords\n}\n\natl08 = earthaccess.search_data(**params)\n\n\nfiles = earthaccess.download(atl08[0:2], \"data\")\nfiles\n\n\n# files = earthaccess.open(atl08[0:5])\nds = xr.open_dataset(files[0], group='/gt1l/signal_photons')\nds\n\n\nds = xr.open_dataset(files[1], group='/gt1l/signal_photons')\nds\n\nhvplot is an interactive plotting tool that is useful for exploring data.\n\nds['ph_h'].hvplot(kind='scatter', s=2)"
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#learning-outcomes-recap",
    "href": "tutorials/data-access/earthaccess.html#learning-outcomes-recap",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "3. Learning outcomes recap",
    "text": "3. Learning outcomes recap\nWe have learned how to: 1. use earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results; 2. open data granules using direct access to the ICESat-2 S3 bucket; 3. load a HDF5 group into an xarray.Dataset; 4. visualize the land ice heights using hvplot."
  },
  {
    "objectID": "tutorials/data-access/earthaccess.html#additional-resources",
    "href": "tutorials/data-access/earthaccess.html#additional-resources",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "4. Additional resources",
    "text": "4. Additional resources\nFor general information about NSIDC DAAC data in the Earthdata Cloud:\nFAQs About NSIDC DAAC’s Earthdata Cloud Migration\nNASA Earthdata Cloud Data Access Guide\nAdditional tutorials and How Tos:\n\nNASA Earthdata Cloud Cookbook\nICESat-2 Hackweek Noteboooks\nOpen Altimetry Portal\nearthaccess slides"
  },
  {
    "objectID": "tutorials/data-access/earthdata-search.html",
    "href": "tutorials/data-access/earthdata-search.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search\nThis tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the GEDI or ICESAT-2 which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nType GEDI in the search bar Click on the “Available from AWS Cloud” filter option on the left.\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here we are using a keyword GEDI.\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\n\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\n\nthe S3 storage bucket and object prefix where this data is located\n\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\n\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nNote: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (ECOSTRESS ECO_L2T_LSTE) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4a. Download or data access for a single granule\nTo download files for a granule click the download arrow on the card (or list row)\n\n\n\nFigure caption: Download granules\n\n\nYou can also get the S3 information (e.g., AWS region, bucket, temperary credentials for S3 access, and file names) by selecting the AWS S3 Access tab.\n\n\n\nFigure caption: S3 access for granules\n\n\n\nStep 4b. Download or data access for multiple granule\nTo download multiple granules, click on the green + symbol to add files to our project. Click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\nOn the next page click the Direct Download option and click the green Download Data on the bottom left side of the page. In this page some datasets will provide a data customization service, this is really important if we don’t want to download the full record but just the variables and region of interest.\n\n\n\nFigure caption: Direct download multiple granules\n\n\nIf we pick the direct download option, we’ll be redirected to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n\n\n\nFigure caption: Download to local\n\n\nIf we select the data customization service we’ll be able to subset by region of interest, variables and temporal parameters.\n\n\n\nFigure caption: Harmony subsetter service\n\n\nVariables can be selected too.\n\n\n\nFigure caption: Harmony subsetter service\n\n\nThe Download Files tab provides the https:// links for downloading the files locally\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nHands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub"
  },
  {
    "objectID": "tutorials/schedule.html",
    "href": "tutorials/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The SSC GEDI/ICESAT-2 Workshop will take place on November 15th.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access."
  },
  {
    "objectID": "tutorials/schedule.html#workshop-schedule",
    "href": "tutorials/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nHour\nEvent\nInstructor\n\n\n\n\n3:00 pm CST/MX\nWelcome and Overview of Tutorial\n\n\n\n3:10 pm\nCryoCloud - A Shared Cloud Platform for NASA\nTasha Snow (Colorado School of Mines)\n\n\n3:25 pm\nHub task - clone repo\n\n\n\n3:35 pm\nBreak (Q&A)\n\n\n\n3:40 pm\nICESat-2 Data Access, NASA DAAC @ NSIDC\nLuis Alberto Lopez Espinosa  (NSIDC, University of Colorado)\n\n\n4:00 pm\nBreak (Q&A)\n\n\n\n4:10 pm\nThe icepyx Software Library and Community\nRachel Wegener (University of Maryland)\n\n\n4:30 pm\nBreak\n\n\n\n4:40 pm\nAccess and Discovery of GEDI Data via the DAAC at the ORNL\nRupesh Shrestha (Oakridge National Laboratory, ORNL-DAAC)\n\n\n5:00 pm\nBreak (Q&A)\n\n\n\n5:10 pm\nGEDI Data User Resources at the LP DAAC\nMahsa Jami (NASA LP-DAAC)\n\n\n5:30 pm\nBreak (Q&A)\n\n\n\n5:40 pm\nEjemplos de aplicaciones de uso de éxito GEDI y posibilidades(Mapas, Reforestacion, Especie protegida)\nAdrian Pascual\n\n\n6:00 pm CST/MX\nEnd of Tutorial\n\n\n\n\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nYou will continue to have access to the 2i2c JupyterHub in AWS for a week following the SSC GEDI/ICESAT-2 Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "tutorials/schedule.html#getting-help-during-the-workshop",
    "href": "tutorials/schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nPlease use the webex chat platform to ask any questions you have during the workshop."
  },
  {
    "objectID": "tutorials/cloud/cloud-paradigm.html",
    "href": "tutorials/cloud/cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "tutorials/science/gedi-data-ssc23.html#helpful-links",
    "href": "tutorials/science/gedi-data-ssc23.html#helpful-links",
    "title": "How to work with GEDI Level 2B V002 Data",
    "section": "Helpful Links",
    "text": "Helpful Links\n\nLP DAAC Website\nLP DAAC GitHub\nGEDI Data Resources GitHub\nGEDI Data Product Pages\nUniversity of Maryland GEDI - Learn more about the GEDI Mission\n\nOpenAltimetry - Learn about GEDI coverage\n\nNASA Earthdata Search"
  },
  {
    "objectID": "tutorials/science/gedi-data-ssc23.html#contact-info",
    "href": "tutorials/science/gedi-data-ssc23.html#contact-info",
    "title": "How to work with GEDI Level 2B V002 Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-15-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html",
    "title": "Demo JupyterHub",
    "section": "",
    "text": "Author: Tasha Snow"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#access-the-cryocloud-powerpoint-whenever-you-need-to-reference-it",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#access-the-cryocloud-powerpoint-whenever-you-need-to-reference-it",
    "title": "Demo JupyterHub",
    "section": "Access the CryoCloud powerpoint whenever you need to reference it",
    "text": "Access the CryoCloud powerpoint whenever you need to reference it\nOpen the powerpoint by directly clicking on the hyperlink above or to open it in the Openscapes Linux Desktop web browser as follows: - Copy this hyperlink: https://bit.ly/4785hMv - Click on the plus (+) sign in the File Browser to the left to open a Launcher window - Under Notebooks, click on Desktop to access the Linux Desktop. This will open a new tab. - Click on the Web Browser tool (globe) at the bottom of the screen - Paste the url into the search bar"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#open-cryocloud",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#open-cryocloud",
    "title": "Demo JupyterHub",
    "section": "Open CryoCloud",
    "text": "Open CryoCloud\n\nScroll through the server sizes. Stick with the 3.7Gb server (the default).\n\n```owumvvvjehru Tip Be realistic about the max memory you will need. The amount you select, you are guaranteed, but if you use more you risk crashing your server for you and anyone else who is sharing with you. If you crash the server, it just requires logging out and reopening it, but it could be annoying for everyone.\nCheck your memory usage at the bottom in the middle of the screen.\n\n2) Choose the Python programming language.\n\n3) Sit back and learn about each of the tools!\n    - JupyterHub options and viewing setup\n    - Github\n    - Virtual Linux desktop\n    - SyncThing\n    - Viewing and editing of different files\n\nNow after the demo...\n\n## Task: Clone the Espacio and Sostenibilidad Colloquium jupyterbook\n\nWe will import the [NASA Openscapes Espacio and Sostenibilidad Colloquium Github repository](https://github.com/NASA-Openscapes/2023-ssc.git).\n\nTo do this: \n1. Select the plus (`+`) sign above the `File Browser` to the left, which will bring up a `Launcher` window. \n\n2. Click the `terminal` button under Other to open it. This is your command line like you would have on any computer. \n\nBefore cloning the repo, you have the option to switch to another file folder using the _change directory_ terminal command: `cd folder` if you do not want the Hackweek repo in your current directory (you can check which directory you are currently in using _print working directory_ command: `pwd`).\ncd yourfoldername\n\n3. Now clone the hackweek code into your current directory: \ngit clone https://github.com/NASA-Openscapes/2023-ssc.git\n\n4. You will see the folder pop into your `File Browser` on the left if you have the current directory open. Click on the folder to navigate through the files. \n\n5. To open this tutorial, click on the `book` subdirectory &gt; `tutorials` &gt; `jupyterhub_demo` &gt; and double click on `jupyterhub_demo`. This should open up this tutorial in case you want to review it in the future. \n\n## Shutting down your JupyterHub\n\n```{admonition} TIP\n**Best Practice: Shut down the Openscapes server when you are done to save us money.**\n\n**If you only close your tab or click log out, your server will continue running for 90 minutes.**\nWhenever you are done, it is best to shut down your server when you sign out to save money. Time on the JupyterHub costs money and there are systems in place to make sure your server doesn’t run indefinitely if you forget about it. After 90 minutes of no use, it will shut down. We prefer you shut down the server when so we save that 90 minutes of computing cost. To do so:\n\nIn upper left, click on File &gt; Hub Control Panel, which will open another tab\nClick the Stop Server button. Once this button disappears after you clicked it, your server is off.\nClick Log Out in the top right of your screen and you will be logged out, or you can start a new server\nYou can now close this tab and the other tab where you were just working"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#summary",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#summary",
    "title": "Demo JupyterHub",
    "section": "Summary",
    "text": "Summary\n🎉 Congratulations! You’ve completed this tutorial and have seen how we can access and use the Openscapes JupyterHub."
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#references",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#references",
    "title": "Demo JupyterHub",
    "section": "References",
    "text": "References\nTo learn more about CryoCloud, gain code for NASA data access, and find other Cryosphere tutorials check out this other documentation:\n\nCryoCloud JupyterBook"
  },
  {
    "objectID": "tutorials/prerequisites.html",
    "href": "tutorials/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "tutorials/prerequisites.html#prerequisites",
    "href": "tutorials/prerequisites.html#prerequisites",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "tutorials/further-resources.html",
    "href": "tutorials/further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "tutorials/further-resources.html#a-growing-list-of-resources",
    "href": "tutorials/further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "tutorials/further-resources.html#additional-tutorials",
    "href": "tutorials/further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html",
    "href": "tutorials/science/gedi-l4a-agbd.html",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "",
    "text": "This tutorial will demonstrate how to directly access and subset the GEDI L4A dataset using NASA’s Harmony Services and compute summary of aboveground biomass for a protected area in Mexico. The Harmony API allows seamless access and production of analysis-ready Earth observation data across different DAACs, by enabling cloud-based spatial, temporal, and variable subsetting and data conversions. The GEDI L4A dataset is available from the Harmony API."
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#overview",
    "href": "tutorials/science/gedi-l4a-agbd.html#overview",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "",
    "text": "This tutorial will demonstrate how to directly access and subset the GEDI L4A dataset using NASA’s Harmony Services and compute summary of aboveground biomass for a protected area in Mexico. The Harmony API allows seamless access and production of analysis-ready Earth observation data across different DAACs, by enabling cloud-based spatial, temporal, and variable subsetting and data conversions. The GEDI L4A dataset is available from the Harmony API."
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#learning-objectives",
    "href": "tutorials/science/gedi-l4a-agbd.html#learning-objectives",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUse NASA’s Harmony Services to retrieve the GEDI L4A dataset. The Harmony API allows access to selected variables for the dataset within the spatial-temporal bounds without having to download the whole data file.\nCompute summaries of AGBD across various plant functional types (PFTs) in the study area."
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#dataset",
    "href": "tutorials/science/gedi-l4a-agbd.html#dataset",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Dataset",
    "text": "Dataset\nThe Global Ecosystem Dynamics Investigation (GEDI) L4A Footprint Level Aboveground Biomass Density (AGBD) dataset provides predictions of the aboveground biomass density (AGBD; in Mg/ha) and estimates of the prediction standard error within each sampled geolocated GEDI footprint. GEDI L4A dataset is available for the period starting 2019-04-17 and covers 52 N to 52 S latitudes. GEDI L4A data files are natively in HDF5 format. GEDI L4A dataset should be cited as: &gt; Dubayah, R.O., J. Armston, J.R. Kellner, L. Duncanson, S.P. Healey, P.L. Patterson, S. Hancock, H. Tang, J. Bruening, M.A. Hofton, J.B. Blair, and S.B. Luthcke. 2022. GEDI L4A Footprint Level Aboveground Biomass Density, Version 2.1. ORNL DAAC, Oak Ridge, Tennessee, USA. doi:10.3334/ORNLDAAC/2056"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#requirements",
    "href": "tutorials/science/gedi-l4a-agbd.html#requirements",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Requirements",
    "text": "Requirements\n\n1. Compute environment\nThis notebook can be run in any personal computing environment (e.g., desktop/laptops), on-premise solution (e.g., High-Performance Computing), or on the Cloud (e.g., Amazon Web Service).\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. Additional Requirements\nWhile NASA’s Harmony services are available directly through RESTful API, we will use Harmony-Py Python library for this tutorial. Harmony-Py provides a friendly interface for integrating with NASA’s Harmony Services. In addition to Harmony-Py, this tutorial requires the following Python modules installed in your system: earthaccess, h5py, requests, datetime, pandas, geopandas, contextily. Requirements are also in requirements.txt. To install the necessary Python modules, you can copy the requirements.txt from this repository and run:\npip install --user h5py, requests, datetime, pandas, geopandas, contextily"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#import-modules",
    "href": "tutorials/science/gedi-l4a-agbd.html#import-modules",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Import Modules",
    "text": "Import Modules\n\nimport h5py\nimport requests as re\nimport pandas as pd\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom glob import glob\nfrom harmony import Client, Collection, Environment, Request\nimport seaborn as sns\nsns.set(style='whitegrid')"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#authentication",
    "href": "tutorials/science/gedi-l4a-agbd.html#authentication",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Authentication",
    "text": "Authentication\nNASA Harmony API requires NASA Earthdata Login (EDL). You can use the earthaccess Python library to set up authetincation. Alternatively, you can also login to harmony_client directly by passing EDL authentication as the following in the Jupyter Notebook itself:\nharmony_client = Client(auth=(\"your EDL username\", \"your EDL password\"))"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#create-harmony-client-object",
    "href": "tutorials/science/gedi-l4a-agbd.html#create-harmony-client-object",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Create Harmony Client object",
    "text": "Create Harmony Client object\nFirst, we create a Harmony Client object. If you are passing the EDL authentication, please do as shown above with auth parameter.\n\nharmony_client = Client()"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#retrieve-concept-id",
    "href": "tutorials/science/gedi-l4a-agbd.html#retrieve-concept-id",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Retrieve Concept ID",
    "text": "Retrieve Concept ID\nNow let’s retrieve the Concept ID of the GEDI L4A dataset. The Concept ID is NASA Earthdata’s unique ID for its dataset.\n\n# GEDI L4A DOI \ndoi = '10.3334/ORNLDAAC/2056'\n\n# CMR API base url\ndoisearch=f'https://cmr.earthdata.nasa.gov/search/collections.json?doi={doi}' \nconcept_id = re.get(doisearch).json()['feed']['entry'][0]['id']\nconcept_id\n\n'C2237824918-ORNL_CLOUD'"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#define-request-parameters",
    "href": "tutorials/science/gedi-l4a-agbd.html#define-request-parameters",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Define Request Parameters",
    "text": "Define Request Parameters\nLet’s create a Harmony Collection object with the concept_id retrieved above. We will also define the GEDI L4A variables of interest and temporal range.\n\ncollection = Collection(id=concept_id)\n# gedi beams\nbeams = ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']\n# gedi variables\nvariables = ['agbd', 'l4_quality_flag', 'elev_lowestmode', 'land_cover_data/pft_class']\n# combine variables and beams\nvariables = [f'/{b}/{v}' for b in beams for v in variables]\ntemporal_range = {'start': datetime(2019, 4, 17), \n                  'stop': datetime(2023, 3, 31)}\n\nWe will use the spatial extent of a La Primavera Biosphere Reserve provided as a GeoJSON file at bosque_primavera.json. Let’s open and plot this file.\n\npoly_json = '../data-access/bosque_primavera.json'\npoly = gpd.read_file(poly_json) \nax=poly.to_crs(epsg=3857).plot(figsize=(10, 4), alpha=0.4, color='blue', edgecolor='red')\nplt.margins(y=0.5, x=1.5)\nctx.add_basemap(ax)"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#create-and-submit-harmony-request",
    "href": "tutorials/science/gedi-l4a-agbd.html#create-and-submit-harmony-request",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Create and Submit Harmony Request",
    "text": "Create and Submit Harmony Request\nNow we can create a Harmony request with variables, temporal range, and bounding box and submit the request using the Harmony client object. We will use the download_all method, which uses a multithreaded downloader and returns a concurrent future. Futures are asynchronous and let us use the downloaded file as soon as the download is complete while other files are still being downloaded.\n\nrequest = Request(collection=collection, \n                  variables=variables, \n                  temporal=temporal_range,\n                  shape=poly_json,\n                  ignore_errors=True)\n\n# submit harmony request, will return job id\nsubset_job_id = harmony_client.submit(request)\n\nprint(f'Processing job: {subset_job_id}')\n\nprint(f'Waiting for the job to finish')\nresults = harmony_client.result_json(subset_job_id, show_progress=True)\n\nprint(f'Downloading subset files...')\nfutures = harmony_client.download_all(subset_job_id, overwrite=False)\nfor f in futures:\n    # all subsetted files have this suffix\n    if f.result().endswith('subsetted.h5'):\n        print(f'Downloaded: {f.result()}')\n            \nprint(f'Done downloading files.')\n\nProcessing job: b7dfe272-7c9d-40d0-af68-71aaa056a5b2\nWaiting for the job to finish\nDownloading subset files...\nGEDI04_A_2020138102210_O08093_03_T04765_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020138102210_O08093_03_T04765_02_002_02_V002_subsetted.h5\nGEDI04_A_2019110215109_O02004_03_T03189_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019110215109_O02004_03_T03189_02_002_02_V002_subsetted.h5\nGEDI04_A_2020055190255_O06812_03_T02072_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020055190255_O06812_03_T02072_02_002_02_V002_subsetted.h5\nGEDI04_A_2020251133701_O09847_03_T08881_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020251133701_O09847_03_T08881_02_002_02_V002_subsetted.h5\nGEDI04_A_2019120085608_O02151_02_T02464_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019120085608_O02151_02_T02464_02_002_02_V002_subsetted.h5\nGEDI04_A_2020160162505_O08438_02_T03734_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020160162505_O08438_02_T03734_02_002_02_V002_subsetted.h5\nGEDI04_A_2020255120326_O09908_03_T10457_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020255120326_O09908_03_T10457_02_002_02_V002_subsetted.h5\nGEDI04_A_2020156175938_O08377_02_T05157_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020156175938_O08377_02_T05157_02_002_02_V002_subsetted.h5\nGEDI04_A_2019141004724_O02472_02_T01852_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019141004724_O02472_02_T01852_02_002_02_V002_subsetted.h5\nGEDI04_A_2021070122719_O12714_03_T10457_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021070122719_O12714_03_T10457_02_002_02_V002_subsetted.h5\nGEDI04_A_2020259103039_O09969_03_T06035_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020259103039_O09969_03_T06035_02_002_02_V002_subsetted.h5\nGEDI04_A_2021066140028_O12653_03_T07611_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021066140028_O12653_03_T07611_02_002_02_V002_subsetted.h5\nGEDI04_A_2021074105432_O12775_03_T08881_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021074105432_O12775_03_T08881_02_002_02_V002_subsetted.h5\nGEDI04_A_2021201084306_O14742_03_T06035_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021201084306_O14742_03_T06035_02_002_02_V002_subsetted.h5\nGEDI04_A_2021025205815_O12022_02_T08003_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021025205815_O12022_02_T08003_02_002_02_V002_subsetted.h5\nGEDI04_A_2020334045434_O11128_03_T06188_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020334045434_O11128_03_T06188_02_002_02_V002_subsetted.h5\nGEDI04_A_2021018000447_O11900_02_T00888_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021018000447_O11900_02_T00888_02_002_02_V002_subsetted.h5\nGEDI04_A_2021021223049_O11961_02_T05310_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021021223049_O11961_02_T05310_02_002_02_V002_subsetted.h5\nGEDI04_A_2019215042821_O03623_03_T04765_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019215042821_O03623_03_T04765_02_002_02_V002_subsetted.h5\nGEDI04_A_2021184062047_O14477_02_T10849_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021184062047_O14477_02_T10849_02_002_02_V002_subsetted.h5\nGEDI04_A_2021062153338_O12592_03_T06188_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021062153338_O12592_03_T06188_02_002_02_V002_subsetted.h5\nGEDI04_A_2021341012934_O16907_03_T01919_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021341012934_O16907_03_T01919_02_002_02_V002_subsetted.h5\nGEDI04_A_2021256015705_O15590_02_T06580_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2021256015705_O15590_02_T06580_02_002_02_V002_subsetted.h5\nGEDI04_A_2022088050647_O18646_03_T03189_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2022088050647_O18646_03_T03189_02_003_01_V002_subsetted.h5\nGEDI04_A_2022305150115_O22018_03_T10457_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2022305150115_O22018_03_T10457_02_003_01_V002_subsetted.h5\nGEDI04_A_2022244150958_O21072_03_T00496_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2022244150958_O21072_03_T00496_02_003_01_V002_subsetted.h5\nGEDI04_A_2023012013558_O23126_02_T10849_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2023012013558_O23126_02_T10849_02_003_01_V002_subsetted.h5\nGEDI04_A_2022251032645_O21173_02_T00888_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2022251032645_O21173_02_T00888_02_003_01_V002_subsetted.h5\nGEDI04_A_2023005131909_O23025_03_T08881_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2023005131909_O23025_03_T08881_02_003_01_V002_subsetted.h5\nGEDI04_A_2022129034426_O19281_02_T02464_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2022129034426_O19281_02_T02464_02_003_01_V002_subsetted.h5\nGEDI04_A_2019320104704_O05256_03_T01919_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019320104704_O05256_03_T01919_02_002_02_V002_subsetted.h5\nGEDI04_A_2023032024707_O23437_03_T03342_02_003_01_V002_subsetted.h5\nDownloaded: GEDI04_A_2023032024707_O23437_03_T03342_02_003_01_V002_subsetted.h5\nGEDI04_A_2019326230359_O05357_02_T02311_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2019326230359_O05357_02_T02311_02_002_02_V002_subsetted.h5\nGEDI04_A_2020024073106_O06324_03_T00496_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020024073106_O06324_03_T00496_02_002_02_V002_subsetted.h5\nGEDI04_A_2020040011719_O06568_03_T04765_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020040011719_O06568_03_T04765_02_002_02_V002_subsetted.h5\nGEDI04_A_2020032042419_O06446_03_T03189_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020032042419_O06446_03_T03189_02_002_02_V002_subsetted.h5\nGEDI04_A_2020047221012_O06690_03_T00496_02_002_02_V002_subsetted.h5\nDownloaded: GEDI04_A_2020047221012_O06690_03_T00496_02_002_02_V002_subsetted.h5\nDone downloading files.\n\n\n [ Processing:  86% ] |###########################################        | [|]\nJob is running with errors.\n [ Processing: 100% ] |###################################################| [|]"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#read-subset-files",
    "href": "tutorials/science/gedi-l4a-agbd.html#read-subset-files",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Read Subset files",
    "text": "Read Subset files\nAll the subsetted files are saved as _subsetted.h5. Let’s read these h5 files into a pandas dataframe.\n\nsubset_df = pd.DataFrame()\nfor subfile in glob('*_subsetted.h5'):\n    hf_in = h5py.File(subfile, 'r')\n    for v in list(hf_in.keys()):\n        if v.startswith('BEAM'):\n            beam = hf_in[v]\n            col_names = []\n            col_val = []\n            # read all variables\n            for key, value in beam.items():\n                # check if the item is a group\n                if isinstance(value, h5py.Group):\n                    # looping through subgroups\n                    for key2, value2 in value.items():\n                        col_names.append(key2)\n                        col_val.append(value2[:].tolist())\n                else:\n                    col_names.append(key)\n                    col_val.append(value[:].tolist())\n\n        # Appending to the subset_df dataframe\n        beam_df = pd.DataFrame(map(list, zip(*col_val)), columns=col_names)\n        subset_df = pd.concat([subset_df, beam_df])\n    hf_in.close()\n# print head of dataframe\nsubset_df.head()\n\n\n\n\n\n\n\n\nagbd\ndelta_time\nelev_lowestmode\nlat_lowestmode_a1\nlon_lowestmode_a1\nshot_number\nl4_quality_flag\npft_class\nshot_number\nlat_lowestmode\nlon_lowestmode\nshot_number\n\n\n\n\n0\n82.577484\n6.644999e+07\n1540.948242\n20.717487\n-103.594671\n65680000300329876\n0\n4\n65680000300329876\n20.717488\n-103.594671\n65680000300329876\n\n\n1\n17.222200\n6.644999e+07\n1539.156738\n20.717087\n-103.594327\n65680000300329877\n0\n4\n65680000300329877\n20.717087\n-103.594327\n65680000300329877\n\n\n2\n11.274220\n6.644999e+07\n1538.657593\n20.716686\n-103.593983\n65680000300329878\n0\n4\n65680000300329878\n20.716686\n-103.593983\n65680000300329878\n\n\n3\n22.247044\n6.644999e+07\n1538.058594\n20.716285\n-103.593640\n65680000300329879\n0\n4\n65680000300329879\n20.716285\n-103.593640\n65680000300329879\n\n\n4\n11.021081\n6.644999e+07\n1538.167603\n20.715884\n-103.593297\n65680000300329880\n0\n4\n65680000300329880\n20.715884\n-103.593297\n65680000300329880"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#quality-filter-and-plot",
    "href": "tutorials/science/gedi-l4a-agbd.html#quality-filter-and-plot",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Quality Filter and Plot",
    "text": "Quality Filter and Plot\nWe can now quality filter the dataset and only retrieve the good quality shots for trees and shrub cover plant functional types (PFTs).\n\n# MCD12Q1 PFT types\npft_legend = {0 : 'Water Bodies', \n              1: 'Evergreen Needleleaf Trees', \n              2: 'Evergreen Broadleaf Trees', \n              3: 'Deciduous Needleleaf Trees',  \n              4: 'Deciduous Broadleaf Trees', \n              5: 'Shrub', \n              6: 'Grass',\n              7: 'Cereal Croplands', \n              8: 'Broadleaf Croplands', \n              9: 'Urban and Built-up Lands', \n              10: 'Permanent Snow and Ice', \n              11: 'Barren', \n              255: 'Unclassified'}\n# creating mask with good quality shots and trees/shrubs pft class\nmask = (subset_df['l4_quality_flag']==1) & (subset_df['pft_class'] &lt;= 5 )\n# create geopandas dtframe\ngdf = gpd.GeoDataFrame(subset_df, geometry=gpd.points_from_xy(subset_df.lon_lowestmode, subset_df.lat_lowestmode))\ngdf.crs=\"EPSG:4326\"\n# ax1=gdf_epsg3857.plot(color='white', alpha=0.3, linewidth=5, )\nax1= gdf.to_crs(epsg=3857)[mask].plot(column='agbd', alpha=0.5, vmax=300, \n                                                      linewidth=0, legend=True, figsize=(12, 4))\nplt.margins(y=0.5, x=1.5)\nplt.title('GEDI L4A AGBD estimates by PFTs in La Primavera Biosphere Reserve')\nctx.add_basemap(ax1)\n\n\n\n\nWe can now plot the distribution of the AGBD by plant functional types (PFTs) for the good quality shots.\n\nplt.figure(figsize=(15,5))\nax = gdf[mask].groupby('pft_class')['agbd'].\\\n            apply(lambda x: sns.histplot(x, label = pft_legend[x.name], kde=True))\nplt.xlabel('agbd (Mg / ha)')\nplt.title('Distribution of GEDI L4A AGBD estimates by PFTs in La Primavera Biosphere Reserve')\nplt.legend()\nplt.show()\n\n\n\n\nLet’s also plot how the AGBD is distributed across elevation ranges for different PFTs.\n\ngdf['elev_bin']=pd.cut(gdf['elev_lowestmode'], bins =range(1000, 2600, 200))\ng = sns.catplot(x = \"elev_bin\", y = \"agbd\", data = gdf[mask], col=\"pft_class\", kind=\"box\")\ng.set_xticklabels(rotation=90)\ng.set_titles(\"{col_name}\")\nfor ax in g.axes.flat:\n    ax.set_title(pft_legend[int(float(ax.get_title()))])\ng.set_axis_labels(\"Elevation (m)\")\nsns.despine(top=False, right=False, left=False, bottom=False, offset=None, trim=False)"
  },
  {
    "objectID": "tutorials/science/gedi-l4a-agbd.html#further-resources",
    "href": "tutorials/science/gedi-l4a-agbd.html#further-resources",
    "title": "Estimating Aboveground Biomass in a Protected Area using GEDI L4A dataset",
    "section": "Further Resources",
    "text": "Further Resources\nAdditional tutorials on discovering, accessing, and using GEDI Level 3 and Level 4 data products are available at https://github.com/ornldaac/gedi_tutorials."
  },
  {
    "objectID": "tutorials/science/intro-xarray.html",
    "href": "tutorials/science/intro-xarray.html",
    "title": "Introduction to xarray… and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#why-do-we-need-xarray",
    "href": "tutorials/science/intro-xarray.html#why-do-we-need-xarray",
    "title": "Introduction to xarray… and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#what-is-xarray",
    "href": "tutorials/science/intro-xarray.html#what-is-xarray",
    "title": "Introduction to xarray… and hvplot",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/science/intro-xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "Introduction to xarray… and hvplot",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#subsetting-and-indexing",
    "href": "tutorials/science/intro-xarray.html#subsetting-and-indexing",
    "title": "Introduction to xarray… and hvplot",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#analysis",
    "href": "tutorials/science/intro-xarray.html#analysis",
    "title": "Introduction to xarray… and hvplot",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim"
  },
  {
    "objectID": "tutorials/science/intro-xarray.html#plot-results",
    "href": "tutorials/science/intro-xarray.html#plot-results",
    "title": "Introduction to xarray… and hvplot",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/cloud/index.html",
    "href": "tutorials/cloud/index.html",
    "title": "CryoCloud / Openscapes",
    "section": "",
    "text": "CryoCloud / Openscapes\nPlaceholder"
  },
  {
    "objectID": "tutorials/setup.html",
    "href": "tutorials/setup.html",
    "title": "Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Workshop folder."
  },
  {
    "objectID": "tutorials/setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\n\nSee how my Desktop is setup\n\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/setup.html#discussion-cloud-environment",
    "href": "tutorials/setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\n\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/setup.html#discussion-my-desktop-setup",
    "href": "tutorials/setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book\nSlack"
  },
  {
    "objectID": "tutorials/setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2022-ECOSTRESS-Cloud-Workshop is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/setup.html#jupyter-notebooks",
    "href": "tutorials/setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/setup.html#how-do-i-end-my-session",
    "href": "tutorials/setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -&gt; Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/data-access/index.html",
    "href": "tutorials/data-access/index.html",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "The purpose of this notebook is to provide an introduction to the land and water data available from the NASA ICESat-2 Mission, along with an overview of data search and access options for the data. We will then provide examples in python for how to search and access the data using the earthaccess and icepyx software libraries.\n\n\n\nThe notebook was created by Rachel Wegener, University of Maryland; Luis Lopez, NSIDC DAAC; and Amy Steiker, NSIDC DAAC.\n\n\n\nBy the end of this demonstration you will be able to:\n\nLearn about the coverage, resolution, and structure of land and water data from the NASA ICESat-2 mission;\nUnderstand the available resources at the National Snow and Ice Data Center (NSIDC), including user support documents, data access options, and data tools and services such as OpenAltimetry;\nUse earthaccess and hvplot to search, access, and visualize ICESat-2 data within a cloud environment.\nUse icepyx to explore and subset variables within ICESat-2 land height files.\n\n\n\n\nSee Prerequisites"
  },
  {
    "objectID": "tutorials/data-access/index.html#tutorial-overview",
    "href": "tutorials/data-access/index.html#tutorial-overview",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "The purpose of this notebook is to provide an introduction to the land and water data available from the NASA ICESat-2 Mission, along with an overview of data search and access options for the data. We will then provide examples in python for how to search and access the data using the earthaccess and icepyx software libraries.\n\n\n\nThe notebook was created by Rachel Wegener, University of Maryland; Luis Lopez, NSIDC DAAC; and Amy Steiker, NSIDC DAAC.\n\n\n\nBy the end of this demonstration you will be able to:\n\nLearn about the coverage, resolution, and structure of land and water data from the NASA ICESat-2 mission;\nUnderstand the available resources at the National Snow and Ice Data Center (NSIDC), including user support documents, data access options, and data tools and services such as OpenAltimetry;\nUse earthaccess and hvplot to search, access, and visualize ICESat-2 data within a cloud environment.\nUse icepyx to explore and subset variables within ICESat-2 land height files.\n\n\n\n\nSee Prerequisites"
  },
  {
    "objectID": "tutorials/data-access/index.html#nasas-icesat-2-mission-land-and-water-products",
    "href": "tutorials/data-access/index.html#nasas-icesat-2-mission-land-and-water-products",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "2. NASA’s ICESat-2 Mission: Land and Water Products",
    "text": "2. NASA’s ICESat-2 Mission: Land and Water Products\n\nIntroduction to ICESat-2\nA quick summary of the ICESat-2 Mission and its instrument, ATLAS: Advanced Topographic Laser Altimeter System:\n\nHeight determined using round-trip travel time of laser light (photon counting lidar)\n10,000 laser pulses released per second, split into 3 weak/strong beam pairs at a wavelength of 532 nanometers (bright green on the visible spectrum).\nMeasurements taken every 70 cm along the satellite’s ground track, roughly 11 m wide footprint.\n\nThe number of photons that return to the telescope depends on surface reflectivity and cloud cover (which obscures ATLAS’s view of Earth). As such, the spatial resolution of signal photons varies.\n\n\n\nICESat-2: Land and Water Products\nThere are over 20 ICESat-2 data products, structured by processing level and surface type:\n\n\n\nICESat-2 land and water products\n\n\nAdditional quick look products are available:\n\nSatellite observations available within 3 days, versus 30-45 days for standard products\nSupports decision-making applications: vegetation height, surface water and flooding, land surface deformation from landslides and volcanoes\nNote that geolocation and height uncertainty is greater for these products (see user guides for more info)\n\nL3A Land and Vegetation Height Quick Look (ATL08QL)\nL3A Along Track Inland Surface Water Data Quick Look (ATL13QL)\nSee https://nsidc.org/data/icesat-2/products for more details.\n\n\nATLAS/ICESat-2 L3A Land and Vegetation Height (ATL08)\nhttps://doi.org/10.5067/ATLAS/ATL08.006\nATL08 contains heights for both terrain and canopy in the along-track direction, plus other descriptive parameters. * Data are derived from geolocated, time-tagged photon heights from the ATLAS/ICESat-2 L2A Global Geolocated Photon Data (ATL03) product.\nSpatial Resolution * Canopy and ground surfaces are processed in fixed 100 m data segments * Each segment typically contain more than 100 signal photons but may contain less.\nTemporal Resolution * 91-day repeat cycle along each of ATLAS/ICESat-2’s 1,387 Reference Ground Tracks (RGTs)\nUncertainty * Expected ranging precision for flat surfaces: Standard deviation of ~25 cm. * Terrain height uncertainty estimates depend on ATL03, and local uncertainties within each 100 m segment\nSee the ATL08 User Guide for more information."
  },
  {
    "objectID": "tutorials/data-access/index.html#overview-of-icesat-2-nsidc-daac-data-tools-and-resources",
    "href": "tutorials/data-access/index.html#overview-of-icesat-2-nsidc-daac-data-tools-and-resources",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "3. Overview of ICESat-2 NSIDC DAAC Data, Tools, and Resources",
    "text": "3. Overview of ICESat-2 NSIDC DAAC Data, Tools, and Resources\n\nOverview of NSIDC data and resources\nThe NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) provides over 800 data sets covering the Earth’s cryosphere and more, all of which are available to the public free of charge. These data can be used to study topics relating to snow cover, sea ice, ice sheets, ice shelves, glaciers, frozen ground, soil moisture, climate interactions, and more. Beyond providing these data, NSIDC creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere.\n\nData Access Tools for ICESat-2\nThere are many tool and access options available for ICESat-2, which can be found on each of the dataset landing pages on NSIDC’s website. See the ATL08 Data Access & Tools section as an example. This notebook will descibe three of those options in more detail: OpenAltimetry, icepyx, and earthaccess.\nUsing OpenAltimetry to Visualize ICESat-2 Data\nKey functions of OpenAltimetry include: * Ground track filtering and visualization * On-the-fly plotting of segment elevations and photon clouds based date and region of interest * Access data in CSV or subsetted HDF5 format * Plot and analyze photon data from your area of interest using a Jupyter Notebook\nUpload a shapefile of interest; view track coverage: \nSelect a region; view elevation profile: \n\n\nNavigating ICESat-2 Tool & Access options:\nThis table provides an overview of the capabilities across the tools and services highlighted in this notebook:\n\n\n\n\nicepyx\nearthaccess\nOpenAltimetry\n\n\n\n\nFilter spatially using:\n\n\n\n\n\n- Interactive map widget\n\nsoon!\nx\n\n\n- Bounding Box\nx\nx\nx\n\n\n- Polygon\nx\nx\n\n\n\n- GeoJSON or Shapefile\nx\nsoon!\nx\n\n\nFilter by time and date\nx\nx\nx\n\n\nPreview data\nx\nx\nx\n\n\nDownload data from DAAC\nx\nx\nx\n\n\nAccess cloud-hosted data\nx\nx\n\n\n\nSubset (spatially, temporally, by variable)\nx\n\nx\n\n\nPlot data with built-in methods\nx\n\nx"
  },
  {
    "objectID": "tutorials/data-access/index.html#search-and-access-of-atl08-using-earthaccess",
    "href": "tutorials/data-access/index.html#search-and-access-of-atl08-using-earthaccess",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "4. Search and access of ATL08 using earthaccess",
    "text": "4. Search and access of ATL08 using earthaccess\nThis notebook demonstrates searching for cloud-hosted ICESat-2 data and directly accessing Land Ice Height (ATL06) granules from an Amazon Compute Cloud (EC2) instance using the earthaccess package. NASA data “in the cloud” are stored in Amazon Web Services (AWS) Simple Storage Service (S3) Buckets. Direct Access is an efficient way to work with data stored in an S3 Bucket when you are working in the cloud. Cloud-hosted granules can be opened and loaded into memory without the need to download them first. This allows you take advantage of the scalability and power of cloud computing.\nThe Amazon Global cloud is divided into geographical regions. To have direct access to data stored in a region, our compute instance - a virtual computer that we create to perform processing operations in place of using our own desktop or laptop - must be in the same region as the data. This is a fundamental concept of analysis in place. NASA cloud-hosted data is in Amazon Region us-west2. So your compute instance must also be in us-west2. If we wanted to use data stored in another region, to use direct access for that data, we would start a compute instance in that region.\nAs an example data collection, we use ATL08, the L3A Land and Vegetation Height dataset from ICESat-2 over Bosque de la Primavera, a protected forest near Guadalajara. ICESat-2 data granules, including ATL08, are stored in HDF5 format. We demonstrate how to open an HDF5 granule and access data variables using xarray and plotting using hvplot."
  },
  {
    "objectID": "tutorials/data-access/index.html#import-packages",
    "href": "tutorials/data-access/index.html#import-packages",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Import Packages",
    "text": "Import Packages\nThe first step in any python script or notebook is to import packages. This tutorial requires the following packages: - earthaccess, which enables Earthdata Login authentication and retrieves AWS credentials; enables collection and granule searches; and S3 access; - xarray, used to load data; - ipyleaflet, used for interactive maps in Jupyter - shapely, used to work with geometric objects - hvplot, used to visualize land ice height data.\nWe are going to import the whole earthaccess package.\nWe will also import the whole xarray package but use a standard short name xr, using the import &lt;package&gt; as &lt;short_name&gt; syntax. We could use anything for a short name but xr is an accepted standard that most xarray users are familiar with.\nWe only need the xarray module from hvplot so we import that using the import &lt;package&gt;.&lt;module&gt; syntax.\nSince we are going to use a developer version of earthaccess, we are going to install it directly from Github, after executing the next cell we need to restart the kernel.\n\n%%capture\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\n# For searching NASA data\nimport earthaccess\n# to work with geometries\nimport shapely\n# to load json (geojson)\nimport json\n# interactive mapping in Python\nimport ipyleaflet\n\n# For reading data, analysis and plotting\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport pprint"
  },
  {
    "objectID": "tutorials/data-access/index.html#authenticate",
    "href": "tutorials/data-access/index.html#authenticate",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Authenticate",
    "text": "Authenticate\nThe first step is to get the correct authentication that will allow us to get cloud-hosted ICESat-2 data. This is all done through Earthdata Login. The login method also gets the correct AWS credentials.\nLogin requires your Earthdata Login username and password. The login method will automatically search for these credentials as environment variables or in a .netrc file, and if those aren’t available it will prompt us to enter our username and password. We use a .netrc strategy. A .netrc file is a text file located in our home directory that contains login information for remote machines. If we don’t have a .netrc file, login can create one for us.\nearthaccess.login(persist=True)\n\nauth = earthaccess.login()\n\nEnter your Earthdata Login username:  earthaccess\nEnter your Earthdata password:  ········"
  },
  {
    "objectID": "tutorials/data-access/index.html#search-for-icesat-2-collections",
    "href": "tutorials/data-access/index.html#search-for-icesat-2-collections",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search for ICESat-2 Collections",
    "text": "Search for ICESat-2 Collections\nearthaccess leverages the Common Metadata Repository (CMR) API to search for collections and granules. Earthdata Search also uses the CMR API.\nWe can use the search_datasets method to search for ICESat-2 collections by setting keyword='ICESat-2'.\nThis will display the number of data collections (data sets) that meet this search criteria.\n\ndatasets = earthaccess.search_datasets(keyword = 'ICESat-2')\n\nDatasets found: 89\n\n\nIn this case there are 65 collections that have the keyword ICESat-2.\nThe search_datasets method returns a python list of DataCollection objects. We can view the metadata for each collection in long form by passing a DataCollection object to print or as a summary using the summary method. We can also use the pprint function to Pretty Print each object.\nWe will do this for the first 10 results (objects).\n\nfor collection in datasets[:3]:\n    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n    print('')\n    \n\n{   'concept-id': 'C2559919423-NSIDC_ECS',\n    'file-type': \"[{'FormatType': 'Native', 'Format': 'HDF5', \"\n                 \"'FormatDescription': 'HTTPS'}]\",\n    'get-data': [   'https://n5eil01u.ecs.nsidc.org/ATLAS/ATL03.006/',\n                    'https://search.earthdata.nasa.gov/search?q=ATL03+V006',\n                    'http://openaltimetry.org/',\n                    'https://nsidc.org/data/data-access-tool/ATL03/versions/6/'],\n    'short-name': 'ATL03',\n    'version': '006'}\n\n{   'cloud-info': {   'Region': 'us-west-2',\n                      'S3BucketAndObjectPrefixNames': [   'nsidc-cumulus-prod-protected/ATLAS/ATL03/006',\n                                                          'nsidc-cumulus-prod-public/ATLAS/ATL03/006'],\n                      'S3CredentialsAPIDocumentationURL': 'https://data.nsidc.earthdatacloud.nasa.gov/s3credentialsREADME',\n                      'S3CredentialsAPIEndpoint': 'https://data.nsidc.earthdatacloud.nasa.gov/s3credentials'},\n    'concept-id': 'C2596864127-NSIDC_CPRD',\n    'file-type': \"[{'FormatType': 'Native', 'Format': 'HDF5', \"\n                 \"'FormatDescription': 'HTTPS'}]\",\n    'get-data': ['https://search.earthdata.nasa.gov/search?q=ATL03+V006'],\n    'short-name': 'ATL03',\n    'version': '006'}\n\n{   'concept-id': 'C2120512202-NSIDC_ECS',\n    'file-type': \"[{'FormatType': 'Native', 'Format': 'HDF5', \"\n                 \"'FormatDescription': 'HTTPS'}]\",\n    'get-data': [   'https://n5eil01u.ecs.nsidc.org/ATLAS/ATL03.005/',\n                    'https://search.earthdata.nasa.gov/search?q=ATL03+V005',\n                    'http://openaltimetry.org/',\n                    'https://nsidc.org/data/data-access-tool/ATL03/versions/5/'],\n    'short-name': 'ATL03',\n    'version': '005'}\n\n\n\nFor each collection, summary returns a subset of fields from the collection metadata and the Unified Metadata Model (UMM): - concept-id is a unique id for the collection. It consists of an alphanumeric code and the provider-id specific to the DAAC (Distributed Active Archive Center). You can use the concept_id to search for data granules. - short_name is a quick way of referring to a collection (instead of using the full title). It can be found on the collection landing page underneath the collection title after ‘DATA SET ID’. See the table below for a list of the shortnames for ICESat-2 collections. - version is the version of each collection. - file-type gives information about the file format of the collection granules. - get-data is a collection of URLs that can be used to access the data, collection landing pages and data tools. - cloud-info this is for cloud-hosted data and provides additional information about the location of the S3 bucket that holds the data and where to get temporary AWS S3 credentials to access the S3 buckets. earthaccess handles these credentials and the links to the S3 buckets, so in general you won’t need to worry about this information.\nFor the ICESat-2 search results, within the concept-id, there is a provider-id; NSIDC_ECS and NSIDC_CPRD. NSIDC_ECS which is for the on-prem collections and NSIDC_CPRD is for the cloud-hosted collections.\nFor ICESat-2, ShortNames are generally how different products are referred to.\n\n\n\n\n\n\n\nShortName\nProduct Description\n\n\n\n\nATL03\nATLAS/ICESat-2 L2A Global Geolocated Photon Data\n\n\nATL06\nATLAS/ICESat-2 L3A Land Ice Height\n\n\nATL07\nATLAS/ICESat-2 L3A Sea Ice Height\n\n\nATL08\nATLAS/ICESat-2 L3A Land and Vegetation Height\n\n\nATL09\nATLAS/ICESat-2 L3A Calibrated Backscatter Profiles and Atmospheric Layer Characteristics\n\n\nATL10\nATLAS/ICESat-2 L3A Sea Ice Freeboard\n\n\nATL11\nATLAS/ICESat-2 L3B Slope-Corrected Land Ice Height Time Series\n\n\nATL12\nATLAS/ICESat-2 L3A Ocean Surface Height\n\n\nATL13\nATLAS/ICESat-2 L3A Along Track Inland Surface Water Data\n\n\n\n\nSearch for cloud-hosted data\nFor most collections, to search for only data in the cloud, the cloud_hosted method can be used.\n\ncloud_hosted_datasets = earthaccess.search_datasets(\n    keyword = 'ICESat-2',\n    cloud_hosted = True\n)\n\nDatasets found: 40"
  },
  {
    "objectID": "tutorials/data-access/index.html#search-a-data-set-using-spatial-and-temporal-filters",
    "href": "tutorials/data-access/index.html#search-a-data-set-using-spatial-and-temporal-filters",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search a data set using spatial and temporal filters",
    "text": "Search a data set using spatial and temporal filters\nWe can use the search_data method to search for granules within a data set by location and time using spatial and temporal filters. In this example, we will search for data granules from the following datasets:\n\nATLAS/ICESat-2 L3A Land and Vegetation Height, Version 6: C2596864127-NSIDC_CPRD\nGEDI L4A Footprint Level Aboveground Biomass Density, Version 2.1: C2237824918-ORNL_CLOUD\nGEDI L2A Elevation and Height Metrics Data Global Footprint Level V002: C1908348134-LPDAAC_ECS\n\nThe temporal range is identified with standard date strings, and latitude-longitude corners of a bounding box is specified. Polygons and points, as well as shapefiles can also be specified. This will display the number of granules that match our search."
  },
  {
    "objectID": "tutorials/data-access/index.html#filtering-data-using-a-region-of-interest-roi",
    "href": "tutorials/data-access/index.html#filtering-data-using-a-region-of-interest-roi",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "5. Filtering data using a region of interest (ROI)",
    "text": "5. Filtering data using a region of interest (ROI)\nSince NASA’s Search API allows us to use polygons for our data queries, we can load a KML, geojson or shapefile and parse the geometry to filter our data. In this example we are going to use Global Forest Watch to download a geojson from Bosque de la Primavera, a protected reserve near Guadalajara Mexico.\n\nwith open(\"bosque_primavera.json\") as f:\n    geojson = json.load(f)\n\ngeojosn_geom = geojson['features'][0]['geometry']\n# polygon = shapely.geometry.box(*bbox, ccw=True)\npolygon = shapely.geometry.shape(geojosn_geom)\npolygon\n\n\n\n\n\nSimplifying geometries with shapely\nUsually, geolocated boundaries contain a lot of points that are depending on the use case not as relevant when we filter data at certain resolutions, for this cases we can simplify our geometries. This also has the advantage of faster spatial queries.\n\n# current geometry\nlen(polygon.boundary.coords)\n\n1527\n\n\n\np = polygon.simplify(0.01, preserve_topology=True)\np\n\n\n\n\n\nlen(p.boundary.coords)\n\n20\n\n\n\npolygon_coords = [(coord[0], coord[1]) for coord in p.boundary.coords]\n\n\nif \"map_widget\" in vars():\n    if len(map_widget.roi) &gt; 0:\n        map_widget = sw.roi\n\nparams = {\n    \"concept_id\" : [\"C2613553260-NSIDC_CPRD\", \"C1908348134-LPDAAC_ECS\", \"C2237824918-ORNL_CLOUD\"],\n    \"temporal\": (\"2022-08\", \"2023-09\"),\n    \"polygon\": polygon_coords\n}\n\nresults = earthaccess.search_data(**params)\n\nGranules found: 27\n\n\nTo display the rendered metadata, including the download link, granule size and two images, we will use display. In the example below, all 4 results are shown.\nThe download link is https and can be used download the granule to your local machine. This is similar to downloading DAAC-hosted data but in this case the data are coming from the Earthdata Cloud. For NASA data in the Earthdata Cloud, there is no charge to the user for egress from AWS Cloud servers. This is not the case for other data in the cloud.\n\npreviews = [display(r) for r in results[0:2]]\n\n\n\n    \n             # TODO: this has to happen just one time like other libraries do it\n            \n    \n      \n        \n          \n            Data: GEDI02_A_2022224135830_O20761_02_T09273_02_003_02_V002.h5\n            Size: 2530.92 MB\n            Cloud Hosted: False\n          \n          \n            \n          \n        \n      \n    \n    \n\n\n\n\n    \n             # TODO: this has to happen just one time like other libraries do it\n            \n    \n      \n        \n          \n            Data: GEDI02_A_2022244150958_O21072_03_T00496_02_003_02_V002.h5\n            Size: 2125.53 MB\n            Cloud Hosted: False"
  },
  {
    "objectID": "tutorials/data-access/index.html#interactive-map-widget",
    "href": "tutorials/data-access/index.html#interactive-map-widget",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Interactive Map Widget",
    "text": "Interactive Map Widget\nAn upcoming feature of earthaccess is the map widget that will allow us to use custom maps for data selection and the exploration or results from NASA’s CMR very similar to what geopandas does with their .explore() method.\n\nmap_widget = earthaccess.search_widget()\n# we add our ROI to the map\nmap_widget.m.add(ipyleaflet.GeoJSON(name=\"ROI\",\n        data=geojosn_geom,\n        style={\n          \"color\": \"red\",\n          \"opacity\": 0.9,\n          \"fillOpacity\": 0.1\n          }\n        ))\n# display the map!\nmap_widget.explore(results)"
  },
  {
    "objectID": "tutorials/data-access/index.html#use-earthaccess-and-xarray-to-download-open-and-plot-data-stored-on-s3",
    "href": "tutorials/data-access/index.html#use-earthaccess-and-xarray-to-download-open-and-plot-data-stored-on-s3",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "6. Use earthaccess and xarray to download, open and plot data stored on S3",
    "text": "6. Use earthaccess and xarray to download, open and plot data stored on S3\nDirect-access to data from an S3 bucket is a two step process. First, the files are opened using the open method. The auth object created at the start of the notebook is used to provide Earthdata Login authentication and AWS credentials.\nThe next step is to load the data. In this case, data are loaded into an xarray.Dataset. Data could be read into numpy arrays or a pandas.Dataframe. However, each granule would have to be read using a package that reads HDF5 granules such as h5py. xarray does this all under-the-hood in a single line but for a single group in the HDF5 granule*.\n*ICESat-2 measures photon returns from 3 beam pairs numbered 1, 2 and 3 that each consist of a left and a right beam. In this case, we are interested in the left ground track (gt) of beam pair 1.\nSince each dataset has different variables and structure, we are going to access and plot ATL08\n\nparams = {\n    \"concept_id\" : [\"C2613553260-NSIDC_CPRD\"],\n    \"temporal\": (\"2022-08\", \"2023-09\"),\n    \"polygon\": polygon_coords\n}\n\natl08 = earthaccess.search_data(**params)\n\nGranules found: 11\n\n\n\nfiles = earthaccess.download(atl08[0:2], \"data\")\nfiles\n\n Getting 2 granules, approx download size: 0.05 GB\n\n\n\n\n\n\n\n\n\n\n\n['data/ATL08_20220826025600_09961601_006_01.h5',\n 'data/ATL08_20220902144459_11101607_006_01.h5']\n\n\n\n# files = earthaccess.open(atl08[0:5])\nds = xr.open_dataset(files[0], group='/gt1l/signal_photons')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (delta_time: 121839)\nCoordinates:\n  * delta_time       (delta_time) datetime64[ns] 2022-08-26T03:00:39.27952320...\nData variables:\n    classed_pc_flag  (delta_time) int8 ...\n    classed_pc_indx  (delta_time) int32 ...\n    d_flag           (delta_time) int8 ...\n    ph_h             (delta_time) float32 ...\n    ph_segment_id    (delta_time) int32 ...\nAttributes:\n    Description:  Contains parameters related to individual photons.\n    data_rate:    Data are stored at the signal-photon classification rate.xarray.DatasetDimensions:delta_time: 121839Coordinates: (1)delta_time(delta_time)datetime64[ns]2022-08-26T03:00:39.279523200 .....long_name :delta timestandard_name :timesource :ATL03contentType :auxiliaryInformationdescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.array(['2022-08-26T03:00:39.279523200', '2022-08-26T03:00:39.280223200',\n       '2022-08-26T03:00:39.280723200', ..., '2022-08-26T03:03:01.734226784',\n       '2022-08-26T03:03:01.737826784', '2022-08-26T03:03:01.739826784'],\n      dtype='datetime64[ns]')Data variables: (5)classed_pc_flag(delta_time)int8...long_name :photon land atbd classificationunits :1source :Land ATBD section 4.10contentType :modelResultdescription :Land Vegetation ATBD classification flag for each photon as either noise, ground, canopy, and top of canopy. 0 = noise,  1 = ground, 2 = canopy, or 3 = top of canopy.flag_meanings :noise ground canopy top_of_canopyflag_values :[0 1 2 3][121839 values with dtype=int8]classed_pc_indx(delta_time)int32...long_name :indicies of classed photonsunits :1source :Retained from prior a_alt_science_ph packetcontentType :referenceInformationdescription :Index  (1-based) of the ATL08 classified signal photon from the start of the ATL03 geolocation segment specified on the ATL08 product at the photon rate in the corresponding parameter, ph_segment_id. This index traces back to specific photon within a 20m segment_id on ATL03.  The unique identifier for tracing each ATL08 signal photon to the corresponding photon record on ATL03 is the segment_id, orbit, cycle, and classed_pc_indx. Orbit and cycle intervals for the granule are found in the /ancillary_data. The timestamp of each orbit transition is found in the /orbit_info group.[121839 values with dtype=int32]d_flag(delta_time)int8...long_name :dragann flagunits :1source :Land ATBD section 2.3.5contentType :qualityInformationdescription :Flag indicating the labeling of DRAGANN noise filtering for a given photon.flag_meanings :noise signalflag_values :[0 1][121839 values with dtype=int8]ph_h(delta_time)float32...long_name :relative photon heightstandard_name :heightunits :meterssource :land/veg ATBD, 15May2020, Section 2.3.4contentType :physicalMeasurementdescription :Height of photons above interpolated land surface[121839 values with dtype=float32]ph_segment_id(delta_time)int32...long_name :segment id of photonunits :1source :Retained from prior a_alt_science_ph packetcontentType :referenceInformationdescription :Segment ID of photons tracing back to specific 20m segment_id on ATL03.  The unique identifier for tracing each ATL08 signal photon to the photon on ATL03 is the segment_id, orbit, and classed_pc_indx. The unique identifier for tracing each ATL08 signal photon to the corresponding photon record on ATL03 is the segment_id, orbit, cycle, and classed_pc_indx. Orbit and cycle intervals for the granule are found in the /ancillary_data. The timestamp of each orbit transition is found in the /orbit_info group.[121839 values with dtype=int32]Indexes: (1)delta_timePandasIndexPandasIndex(DatetimeIndex(['2022-08-26 03:00:39.279523200',\n               '2022-08-26 03:00:39.280223200',\n               '2022-08-26 03:00:39.280723200',\n               '2022-08-26 03:00:39.281023200',\n               '2022-08-26 03:00:39.281923200',\n               '2022-08-26 03:00:39.282723232',\n               '2022-08-26 03:00:39.282823200',\n               '2022-08-26 03:00:39.283023168',\n               '2022-08-26 03:00:39.283923200',\n               '2022-08-26 03:00:39.283923200',\n               ...\n               '2022-08-26 03:03:01.721726784',\n               '2022-08-26 03:03:01.723426784',\n               '2022-08-26 03:03:01.725526784',\n               '2022-08-26 03:03:01.728026816',\n               '2022-08-26 03:03:01.728326752',\n               '2022-08-26 03:03:01.730726816',\n               '2022-08-26 03:03:01.734026784',\n               '2022-08-26 03:03:01.734226784',\n               '2022-08-26 03:03:01.737826784',\n               '2022-08-26 03:03:01.739826784'],\n              dtype='datetime64[ns]', name='delta_time', length=121839, freq=None))Attributes: (2)Description :Contains parameters related to individual photons.data_rate :Data are stored at the signal-photon classification rate.\n\n\nhvplot is an interactive plotting tool that is useful for exploring data.\n\nclassification = np.unique(ds[\"classed_pc_flag\"])\nds['ph_h'].hvplot.scatter(x=\"\", y=\"\", c=ds['classed_pc_flag'].values, cmap=\"tab10\", s=2,)\n\nclassification\n\narray([0, 1, 2, 3], dtype=int8)\n\n\n\nlabels = list(zip(classification, [\"noise\", \"ground\", \"canopy\", \"top_of_canopy\"]))\nlabels\n\n[(0, 'noise'), (1, 'ground'), (2, 'canopy'), (3, 'top_of_canopy')]\n\n\n\npdf = ds.to_pandas()\n\n\npdf.hvplot.scatter(x=\"delta_time\", y=\"ph_h\", by=\"classed_pc_flag\",  cmap=\"tab10\", s=2, width=800)\n\n\n\n\n\n  \n\n\n\n\n\nds['ph_h'].hvplot.scatter(x=\"delta_time\", y=\"ph_h\", c=ds['classed_pc_flag'].values,  cmap=\"tab10\", s=2, width=800)"
  },
  {
    "objectID": "tutorials/data-access/index.html#learning-outcomes-recap",
    "href": "tutorials/data-access/index.html#learning-outcomes-recap",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "7. Learning outcomes recap",
    "text": "7. Learning outcomes recap\nWe have learned how to:\n\nuse earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results;\nopen data granules using direct access to the ICESat-2 S3 bucket;\nload a HDF5 group into an xarray.Dataset;\nvisualize the land ice heights using hvplot."
  },
  {
    "objectID": "tutorials/data-access/index.html#additional-resources",
    "href": "tutorials/data-access/index.html#additional-resources",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "8. Additional resources",
    "text": "8. Additional resources\nFor general information about NSIDC DAAC data in the Earthdata Cloud:\nFAQs About NSIDC DAAC’s Earthdata Cloud Migration\nNASA Earthdata Cloud Data Access Guide\nAdditional tutorials and How Tos:\n\nNASA Earthdata Cloud Cookbook\nICESat-2 Hackweek Noteboooks\nOpen Altimetry Portal\nearthaccess slides"
  },
  {
    "objectID": "tutorials/data-access/icepyx.html",
    "href": "tutorials/data-access/icepyx.html",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "",
    "text": "!pip install icepyx==0.8.1 -qq"
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#what-is-icesat-2",
    "href": "tutorials/data-access/icepyx.html#what-is-icesat-2",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "What is ICESat-2?",
    "text": "What is ICESat-2?\n\n\n\nIS2\n\n\nICESat-2 carries a satellite lidar instrument, ATLAS. Lidar is an active remote sensing technique in which pulses of light are emitted and the return time is used to measure distance. The available ICESat-2 data products range from sea ice freeboard to land elevation to cloud backscatter characteristics. A list of availble products can be found here. In this tutorial we will look at the ATL08 Land Water Vegetation Elevation product.\n\nData Collection\nICESat-2 measures data along 3 strong/weak beam pairs, resulting in 3 strong beams and 3 weak beams. The strong and weak beams are calibrated such that the weak beams have more sensitivity to viewing very bright surfaces (Ex. ice), while the strong beams are able to view surfaces with lower reflectances (Ex. water). The beams are designated in each data product as gt1l, gt1r, gt2l, gt2r, gt3l, and gt3r, where gt stands for “ground track”, the number refers to the photon emitter, and the l and r indicate “left” or “right” beam of the pair. Which of these designations is strong or weak depends on the orientation of the satellite (forwards, sc_orient==1; backwards, sc_orient==0). A helpful table of which beams are strong/weak can be found on p131 of the ATL03 Algorithm Theoretical Basis Document. The ATLAS spot number (values 1-6) is based on the ground track designation (gt1l etc.) and spacecraft orientation and, once determined can be used to consistently identify strong (Spots 1, 3, and 5) and weak (Spots 2, 4, and 6) beams.\n\n\n\nTracks\n\n\nPhoto: Neuenschwander et. al. 2019, Remote Sens. Env. DOI\n\n\nCounting Photons\nThe ICESat-2 lidar collects at the single photon level, different from most commercial lidar systems. A lot of additional photons get returned as solar background noise, and removing these unwanted photons is a key part of the algorithms that produce the higher level data products.\n\n\nFig. 2. Results from signal finding methods for simulated ATLAS data. Black points show raw point cloud data as ingested from ATL03 product. Blue points overlaid in each plot show which photons each method identified as signal. Top panel reflects the signal photons as identified on the ATL03 data product (medium and high confidence signal photons). Bottom panel reflects the signal photons identified from the ATL08 DRAGANN method. (Neuenschwander & Pitts, 2019) Photo: Neuenschwander et. al. 2019, Remote Sens. Env. DOI\n\nTo aggregate all these photons into more manegable chunks ATL08 consolidates the photons into 100m segments, each made up of five 20m segments."
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#what-is-icepyx",
    "href": "tutorials/data-access/icepyx.html#what-is-icepyx",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "What is icepyx?",
    "text": "What is icepyx?\n\nicepyx is a community and software library for searching, downloading, and reading ICESat-2 data. While opening data should be straightforward, there are some oddities in navigating the highly nested organization and hundreds of variables of the ICESat-2 data. icepyx provides tools to help with those oddities.\n\nFitting icepyx into the data access package landscape\nFor ICESat-2 data, the icepyx package can: - search for available data granules (data files) - order and download data - order a subset of data: clipped in space, time, containing fewer variables, or a few other options provided by NSIDC - provides functionality to search through the available data variables - read ICESat-2 data into xarray DataArrays, including merging data from multiple files\n\nimport icepyx as ipx\n\n\nimport json\n\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import shape, GeometryCollection\n\n\n%matplotlib inline\n\n\n\nUsing icepyx to search for data\nWe won’t dive into using icepyx to search for and download data in this tutorial, since we already discussed how to do that with earthaccess. The code to search and download is still provided below for the curious reader. The icepyx documentation shows more detail about different search parameters and how to inspect the results of a query.\n\n# Open a geojson of our area of interest\nwith open(\"bosque_primavera.json\") as f:\n    features = json.load(f)[\"features\"]\n\nbosque = GeometryCollection([shape(feature[\"geometry\"]).buffer(0) for feature in features])\n\n\n# Use our search parameters to setup a search Query\nshort_name = 'ATL08'\nspatial_extent = list(bosque.bounds)\ndate_range = ['2019-05-04','2019-05-04']\nregion = ipx.Query(short_name, spatial_extent, date_range)\n\n\n# Display if any data files, or granules, matched our search\nregion.avail_granules(ids=True)\n\n\n# Download the granules to a into a folder called 'bosque_primavera_ATL08'\nregion.download_granules('./bosque_primavera_ATL08')\n\n\nTip: If you don’t want to type your earthdata login information every time they are required you can setup more automatic methods of authentication. Two common methods are 1) Add your earthdata password and username to as environment variables as EARTHDATA_USERNAME and EARTHDATA_PASSWORD. 2) setup a .netrc file in your home directory. See  the Openscapes tutorial"
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#reading-a-file-with-icepyx",
    "href": "tutorials/data-access/icepyx.html#reading-a-file-with-icepyx",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "Reading a file with icepyx",
    "text": "Reading a file with icepyx\nTo read a file with icepyx there are several steps: 1. Create a Read object. This sets up an initial connection to your file(s) and validates the metadata. 2. Tell the Read object what variables you would like to read 3. Load your data!\n\nCreate a Read object\n\npattern = \"processed_ATL{product:2}_{datetime:%Y%m%d%H%M%S}_{rgt:4}{cycle:2}{orbitsegment:2}_{version:3}_{revision:2}.h5\"\nreader = ipx.Read('./bosque_primavera_ATL08', \"ATL08\", pattern)\n\n\nreader\n\n\n\nSelect your variables\nTo view the variables contained in your dataset you can call .vars on your data reader.\n\nreader.vars.avail()\n\nThats a lot of variables!\nOne key feature of icepyx is the ability to browse the variables available in the dataset. There are typically hundreds of variables in a single dataset, so that is a lot to sort through! Let’s take a moment to get oriented to the organization of ATL08 variables, by first a few important pieces of the algorithm.\nTo create higher level variables like canopy or terrain height, the ATL08 algorithms goes through a series of steps: 1. Identify signal photons from noise photons 2. Classify each of the signal photons as either terrain, canopy, or canopy top 3. Remove elevation, so the heights are with respect to the ground 3. Group the signal photons into 100m segments. If there are a sufficient number of photons in that group, calculate statistics for terrain and canopy (ex. mean height, max height, standard deviation, etc.)\n\n\nFig. 4. An example of the classified photons produced from the ATL08 algorithm. Ground photons (red dots) are labeled as all photons falling within a point spread function distance of the estimated ground surface. The top of canopy photons (green dots) are photons that fall within a buffer distance from the upper canopy surface, and the photons that lie between the top of canopy surface and ground surface are labeled as canopy photons (blue dots). (Neuenschwander & Pitts, 2019)\n\nProviding all the potentially useful information from all these processing steps results in a data file that looks like:\n\nAnother way to visualize these structure is to download one file and open it using https://myhdf5.hdfgroup.org/.\nFurther information about each one of the variables is available in the Algorithm Theoretical Basis Document (ATBD) for ATL08.\nThere is lots to explore in these variables, but we will move forward using a common ATL08 variable: h_canopy, or the “98% height of all the individual relative canopy heights (height above terrain)” (ATBD definition).\n\nreader.vars.append(var_list=['h_canopy', 'latitude', 'longitude'])\n\nNote that adding variables is a required step before you can load the data.\n\n\nLoad the data!\n\nds = reader.load()\nds\n\nHere we have an xarray Dataset, a common Python data structure for analysis. To visualize the data we can plot it using:\n\nds.plot.scatter(x=\"longitude\", y=\"latitude\", hue=\"h_canopy\")\n\nNotice also that the data is shown for just our area of interest! That is because of icepyx’s subsetting feature, which we will discuss more in the next section."
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#using-icepyx-to-download-a-subset-of-a-granule",
    "href": "tutorials/data-access/icepyx.html#using-icepyx-to-download-a-subset-of-a-granule",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "Using icepyx to download a subset of a granule",
    "text": "Using icepyx to download a subset of a granule\nOne feature which is not yet available in earthaccess is the ability to download just a subset of the file. This could mean a smaller spatial area or fewer variables. This feature is available in icepyx.\nWe saw above that icepyx by default will subset your data to the bounding box you provided when downloading. If you know in advance which variables you want icepyx can also subset variables.\n\nSubset variables\n\n# Create our Query\nshort_name = 'ATL08'\nspatial_extent = list(bosque.bounds)\ndate_range = ['2019-05-04','2019-05-04']\nregion = ipx.Query(short_name, spatial_extent, date_range)\n\n\n# Specify desired variables\nregion.order_vars.append(var_list=['h_canopy', 'latitude', 'longitude'])\n\n\n# Download the granules, using the Coverage kwarg to specify variables\nregion.download_granules(path='./ATL08_h_canopy', Coverage=region.order_vars.wanted)\n\n\n# Read the new data\npattern = \"processed_ATL{product:2}_{datetime:%Y%m%d%H%M%S}_{rgt:4}{cycle:2}{orbitsegment:2}_{version:3}_{revision:2}.h5\"\nreader = ipx.Read('./ATL08_h_canopy', \"ATL08\", pattern)\n\nThe available variables list on the subset dataset is a lot shorter!\n\nreader.vars.avail()"
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#some-example-plots",
    "href": "tutorials/data-access/icepyx.html#some-example-plots",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "Some example plots",
    "text": "Some example plots\nIn this last section there are a few more examples of selecting ATL08 variables and plotting them to view the data.\n\nExample 1: View the photon classifications\n\n# Set up the data reader\npattern = \"processed_ATL{product:2}_{datetime:%Y%m%d%H%M%S}_{rgt:4}{cycle:2}{orbitsegment:2}_{version:3}_{revision:2}.h5\"\nreader = ipx.Read('./bosque_primavera_ATL08', \"ATL08\", pattern)\n\n\n# Add the photon height and classification variables\nreader.vars.append(var_list=['ph_h', 'classed_pc_flag', 'latitude', 'longitude'])\n\n\n# load the dataset\nds_photons = reader.load()\nds_photons\n\n\n# plot\nfig, ax = plt.subplots()\n\nds_photons.plot.scatter(ax=ax, x='delta_time', y='ph_h', hue='classed_pc_flag')\n\n\n\nPlot the canopy compared to the ground height\nA nice idea, but there are a few places where the ground may be above the canopy. Not sure how to talk about that. Maybe consider if h_te_best_fit is the best variable to use for height?\n\n# Remove our previous variables\nreader.vars.remove(all=True)\n# Add the next set of variables to the list\nreader.vars.append(var_list=['h_te_best_fit', 'latitude', 'longitude'])\n\n\n# load the data\nds_te = reader.load()\nds_te\n\n\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 3)\n\n# plot the canopy height above ground level\n(ds.h_canopy + ds_te.h_te_best_fit).plot.scatter(ax=ax, x=\"delta_time\", y=\"h_canopy\") # orange\n\n# plot the terrain values\nds_te.plot.scatter(ax=ax, x=\"delta_time\", y=\"h_te_best_fit\") # blue"
  },
  {
    "objectID": "tutorials/data-access/icepyx.html#other-ideas-for-a-fun-last-thing-but-will-likely-get-dropped",
    "href": "tutorials/data-access/icepyx.html#other-ideas-for-a-fun-last-thing-but-will-likely-get-dropped",
    "title": "Using icepyx to access ICESat-2 data",
    "section": "Other ideas for a fun last thing but will likely get dropped",
    "text": "Other ideas for a fun last thing but will likely get dropped\nTrying to come up with a fun final plot, if there is time. Ignore this for now and revisit in the morning.\n\nAdd more of a basemap to the spatial plot\nCartopy doesn’t seem to have any nice built in basemaps, and I think foluim would be too complicated.\n\nfig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n\n# Add background features\nax.add_feature(cfeature.COASTLINE, alpha=0.3)\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.add_feature(cfeature.RIVERS)\n\n# ax.set_extent([-77.5, -75.4, 36.6, 39.7])\n\n# Add and format gridlines. Remove top and right labels\ngl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n                  linewidth=1, color='gray', alpha=0.2, linestyle='--')\ngl.top_labels, gl.right_labels = False, False\n\nds.plot.scatter(ax=ax, x=\"longitude\", y=\"latitude\", hue=\"h_canopy\")\n\nNotice that the data is already subset to our area of interest! (Also download the full granule with earthaccess to compare the area?)\n\n\nInteractive plot with holoviz\n\nimport hvplot.xarray\n\n\nds.hvplot.scatter(y=\"h_canopy\", x=\"latitude\",\n                  by=['spot', 'photon_idx', 'gran_idx'], legend=False)\n\n\ngt1l = ds.where(ds.gt == 'gt1l')"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECOSTRESS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport xarray as xr\nimport geopandas\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\nfrom pyproj import Proj\nimport hvplot.xarray\nimport holoviews as hv\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the ECOSTRESS Tiled Land Surface Temperature and Emissivity data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the S3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\n#s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif'\ns3_url_lst = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\ns3_url_qa = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif'"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the ECOSTRESS Tiles LST S3 URL into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url_lst, chunks='auto')\nda\n\n\nqa = rioxarray.open_rasterio(s3_url_qa, chunks='auto')\nqa\n\n\nLST_dataset = xr.Dataset({'LST': da, 'quality': qa})\nLST_dataset\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = LST_dataset.squeeze('band', drop=True)\nda_lst\n\nPlot the dataarray, representing the LST, using hvplot.\n\nda_lst['LST'].hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, tiles='EsriImagery', width=800, height=600, colorbar=True, title = 'Land Surface Temperature')"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Define the Region of Interest for Clipping",
    "text": "Define the Region of Interest for Clipping\nWe’ll read in our GeoJSON file of our points of interest and create bounding box that contains a points coordinates\n\nfield = geopandas.read_file('../../tutorials/landcover.geojson')\n\nExtract the min/max values for the y and x axis\n\nminx, miny, maxx, maxy = field.geometry.total_bounds\nminx, miny, maxx, maxy\n\nOrder the coordinates for the bounding box counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\nCreate a shapely polygon\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=700, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField \n\nLet’s take a look at the bounding coordinate values.\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner and upper right corner respectively.\n\nfeature_shape.bounds\n\nGet the projection information from the ECOSTRESS file\n\nsrc_proj = da_lst.rio.crs\nsrc_proj\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, src_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, feature_shape)\nfsUTM.bounds\n\nThe coordinates for our feature have now been converted to source raster projection. Note the difference in the values between feature_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our ECOSTRESS LST file to our region of insterest!"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Access and clip the ECOSTRESS LST COG",
    "text": "Access and clip the ECOSTRESS LST COG\nWe can now use our transformed ROI bounding box to clip the ECOSTRESS S3 object we accessed before. We’ll use the rio.clip\n\nda_lst_clip = rioxarray.open_rasterio(s3_url_lst, chunks='auto').squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nda_lst_clip\n\n\nda_lst_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, width=800, height=600,title = 'Land Surface Temperature (Kelvin)', colorbar=True)\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "title": "Data discovery with NASA’s CMR",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS version 2 collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#summary",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#summary",
    "title": "Data discovery with NASA’s CMR",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS version 2 collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#learning-objectives",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#learning-objectives",
    "title": "Data discovery with NASA’s CMR",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what CMR/CMR API is and what CMR/CMR API can be used for\nHow to use the requests package to search data collections and granules\nHow to parse the results of these searches."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-cmr",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-cmr",
    "title": "Data discovery with NASA’s CMR",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "title": "Data discovery with NASA’s CMR",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "title": "Data discovery with NASA’s CMR",
    "section": "Getting Started: How to search CMR from Python",
    "text": "Getting Started: How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/collections'\n\n\nIn this first example, we want to retrieve a list of ECOSTRESS collections in the Earthdata Cloud. This includes ECOSTRESS collections for built 7.1 data which recently became publicly available. This means you will not need to generate a token to access data. Before the public release you should have been part of the access list to access the data. Because of that, an extra token parameter, generated using your Earthdata Login credentials needed to be passed in each CMR request that indicated you are a valid user.\nwe want to retrieve the collections that are hosted in the cloud ('cloud_hosted': 'True') that has granules availble ('has_granules': 'True'). We also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json ('Accept': 'application/json').\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n&lt;Response [200]&gt;\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 4204\nConnection: keep-alive\nDate: Tue, 15 Nov 2022 18:08:14 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: cf80d8ad-a428-4ca4-a85e-998ec5b0c02f\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,23600.0,\"SENTINEL-1A_DP_META_GRD_HIGH\",\"1\",1214470576,826]\nCMR-Hits: 1674\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 237\nX-Request-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 aa0280f933863b8ffd5ff636330f4170.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'1674'\n\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\n'1674'\n\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request. We’ll also assign the term ‘ECOSTRESS’ to a varible so we don’t need to repeatedly add it to the requests parameters.\n\nprovider = 'LPCLOUD'\nproject = 'ECOSTRESS'\n\n\nheaders = {\n    #'Authorization': f'Bearer {token}',\n    'Accept': 'application/json',\n}\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                        },\n                        headers=headers\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.headers['cmr-hits']\n\n'5'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2022-11-15T18:08:14.505Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD&project=ECOSTRESS\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"updated\":\"2021-06-23T16:50:51.108Z\",\"dataset_id\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2T_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website.\\\\r\\\\nThe ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. This tiled data product is subset from the ECO_L2G_LSTE data product using a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\\\\r\\\\nThe ECO_L2T_LSTE Version 2 data product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. This product contains seven layers including LST, LST error, wideband emissivity, quality flags, height, and cloud and water masks. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4, and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS, as mentioned in the Known Issues section. LST data generated after May 15, 2019 will only use the 3 available bands, accuracy may be affected when compared to the LST data that utilized all 5 bands.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076090826-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_GEO\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Geolocation Instantaneous Level 1B Global (ECO_L1B_GEO) Version 2 data product provides the geolocation information for the radiance values retrieved in the ECO_L1B_RAD (https://doi.org/10.5067/ecostress/eco_l1b_rad.002) Version 2 data product. The geolocation product gives geo-tagging to each of the radiance pixels. The geolocation processing corrects the ISS-reported ephemeris and attitude data by image matching with a global ortho-base derived from Landsat data, and then assigns latitude and longitude values to each of the Level 1 radiance pixels. When image matching is successful, the data are geolocated to better than 50 meter (m) accuracy. The ECO_L1B_GEO data product is provided as swath data.\\\\r\\\\n\\\\r\\\\nThe ECO_L1B_GEO data product contains data layers for latitude and longitude values, solar and view geometry information, surface height, and the fraction of pixel on land versus water distributed in HDF5 format.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076087338-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076087338-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"700.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_RAD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m (ECO_L1B_RAD) Version 2 data product provides at-sensor calibrated radiance values retrieved for five thermal infrared (TIR) bands operating between 8 and 12.5 \\xc2\\xb5m. Additionally, the digital numbers (DN) for the shortwave infrared (SWIR) band are provided. The TIR bands are spatially co-registered to produce a variable spatial resolution between 70 meters (m) and 90 m at the edge of the swath. The ECO_L1B_RAD data product is provided as swath data and does not contain geolocation information. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L1B_RAD data product. The geographic coverage of acquisitions for the ECO_L1B_RAD Version 2 data product extends to areas outside of those indicated on the coverage map. \\\\r\\\\n\\\\r\\\\nThe ECO_L1B_RAD Version 2 data product contains layers of radiance values for the five TIR bands, DN values for the SWIR band, associated data quality indicators, and ancillary data distributed in HDF5 format. For acquisitions after May 15, 2019, data products contain data values for the 8.785 \\xce\\xbcm, 10.522 \\xce\\xbcm, and 12.001 \\xce\\xbcm (TIR) bands only. The 1.6 \\xce\\xbcm (SWIR), 8.285 \\xce\\xbcm (TIR), and 9.060 \\xce\\xbcm (TIR) bands contain fill values to accommodate direct streaming of data from the ISS.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076116385-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076116385-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_RAD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_CLOUD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m (ECO_L2_CLOUD) Version 2 data product is derived using a single-channel Bayesian cloud threshold with a look-up-table (LUT) approach. The ECOSTRESS Level 2 cloud product provides a cloud mask that can be used to determine cloud cover for accurate land surface temperature and evapotranspiration estimation. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_CLOUD data product.\\\\r\\\\n \\\\r\\\\nThe ECO_L2_CLOUD Version 2 data product contains three cloud mask layers: brightness temperature LUT test, brightness temperature difference test, and final cloud mask. Information on how to interpret the bit fields in the cloud mask is provided in Table 7 of the User Guide (https://lpdaac.usgs.gov/documents/1493/ECOL2_User_Guide_V2.pdf).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076115306-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076115306-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_CLOUD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1496/ECOL2_ATBD_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"1.5MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m (ECO_L2_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. The ECO_L2_LSTE is provided as swath data and has a spatial resolution of 70 meters (m). The corresponding  ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_LSTE data product.\\\\r\\\\n\\\\r\\\\nThe ECO_L2_LSTE Version 2 data product contains layers of LST, emissivity for bands 1 through 5, quality control for LST&E, LST error, emissivity error for bands 1 through 5, wideband emissivity, Precipitable Water Vapor (PWV), cloud mask, and water mask. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4 and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS as mentioned in the Known Issues section.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076114664-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076114664-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"length\":\"150.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'cloud_hosted': True,\n 'collection_data_type': 'SCIENCE_QUALITY',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2076090826-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/JPL/ECOSTRESS'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['ISS'],\n 'processing_level_id': '2',\n 'score': 1.32,\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'ECO_L2T_LSTE',\n 'summary': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space '\n            'Station (ECOSTRESS) mission measures the temperature of plants to '\n            'better understand how much water plants need and how they respond '\n            'to stress. ECOSTRESS is attached to the International Space '\n            'Station (ISS) and collects data over the conterminous United '\n            'States (CONUS) as well as key biomes and agricultural zones '\n            'around the world and selected FLUXNET validation sites. A map of '\n            'the acquisition coverage can be found on the ECOSTRESS '\n            'website.\\r\\n'\n            'The ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n            'Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data '\n            'product provides atmospherically corrected land surface '\n            'temperature and emissivity (LST&E) values derived from five '\n            'thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was '\n            'derived using a physics-based Temperature/Emissivity Separation '\n            '(TES) algorithm. This tiled data product is subset from the '\n            'ECO_L2G_LSTE data product using a modified version of the '\n            'Military Grid Reference System (MGRS) which divides Universal '\n            'Transverse Mercator (UTM) zones into square tiles that are 109.8 '\n            'km by 109.8 km with a 70 meter (m) spatial resolution.\\r\\n'\n            'The ECO_L2T_LSTE Version 2 data product is provided in Cloud '\n            'Optimized GeoTIFF (COG) format, and each band is distributed as a '\n            'separate COG. This product contains seven layers including LST, '\n            'LST error, wideband emissivity, quality flags, height, and cloud '\n            'and water masks. For acquisitions after May 15, 2019, data '\n            'products contain data values for TIR bands 2, 4, and 5 only. TIR '\n            'bands 1 and 3 contain fill values to accommodate direct streaming '\n            'of data from the ISS, as mentioned in the Known Issues section. '\n            'LST data generated after May 15, 2019 will only use the 3 '\n            'available bands, accuracy may be affected when compared to the '\n            'LST data that utilized all 5 bands.',\n 'time_start': '2018-07-09T00:00:00.000Z',\n 'title': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n          'Instantaneous L2 Global 70 m V002',\n 'updated': '2021-06-23T16:50:51.108Z',\n 'version_id': '002'}\n\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\n\n\nWe know from CMR-Hits that there are 5 datasets but in some situations CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000. If I only search for datasets that are distributed by LPCLOUD provider, we will have more number of results. We can set the page_size parameter to 50 (higher than the number of results returned) so we return all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            #'project': project,\n                            'page_size': 50\n                        },\n                        headers=headers\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.headers['cmr-hits']\n\n'41'\n\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | HLSS30 |C2021957295-LPCLOUD\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | HLSL30 |C2021957657-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | ASTGTM |C1711961296-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MYD11_L2 |C2343114808-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MOD13Q1 |C1748066515-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MOD11A2 |C2269056084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MOD13A3 |C2327962326-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MOD09GA |C2202497474-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MOD11A1 |C1748058432-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MYD13Q1 |C2307290656-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MYD11A1 |C1748046084-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Albedo Daily L3 Global - 500m V061 | MCD43A3 |C2278860820-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MOD09A1 |C2343111356-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MOD09GQ |C2343115666-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MYD09GA |C2202498116-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MOD14 |C2271754179-LPCLOUD\nLP DAAC | MODIS/Terra Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MOD15A2H |C2218777082-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MYD14 |C2278858993-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration 8-Day L4 Global 500m SIN Grid V061 | MOD16A2 |C2343113232-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MYD09GQ |C2343109950-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 4-Day L4 Global 500m SIN Grid V061 | MCD15A3H |C2343110937-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Daily L3 Global - 500m V061 | MCD43A4 |C2218719731-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MOD09Q1 |C2343112831-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V061 | MCD12Q1 |C2484079608-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MYD09A1 |C2343113743-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MYD11A2 |C2269057787-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Aerosol Optical Depth Daily L2G Global 1km SIN Grid V061 | MCD19A2 |C2324689816-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MYD09Q1 |C2343114343-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MYD13A3 |C2327957988-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 0.05Deg CMG V061 | MCD12C1 |C2484078896-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Dynamics Yearly L3 Global 500m SIN Grid V061 | MCD12Q2 |C2484079943-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MCD15A2H |C2222147000-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C2 |C2484081543-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MOD11_L2 |C2343115255-LPCLOUD\nLP DAAC | ECOSTRESS Swath Attitude and Ephemeris Instantaneous L1B Global V002 | ECO_L1B_ATT |C2076117996-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Downward Shortwave Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C1 |C2484081120-LPCLOUD"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#searching-for-granules",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#searching-for-granules",
    "title": "Data discovery with NASA’s CMR",
    "section": "Searching for Granules",
    "text": "Searching for Granules\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO_L2T_LSTE version 2 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\ncollection_id = 'C2076090826-LPCLOUD'\ndate_range = '2022-10-20T00:00:00Z,2022-11-14T23:59:59Z'\nbbox = '-120.295181,34.210026,-119.526215,35.225021'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            #'token': token,\n                            'page_size': 200\n                            },\n                        headers=headers\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n47\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"title\"]} | {granule[\"id\"]}')\n\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01 | G2530780237-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGC_20221026T105945_0710_01 | G2530780962-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGD_20221026T105945_0710_01 | G2530781111-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGD_20221026T110036_0710_01 | G2530775818-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGE_20221026T110036_0710_01 | G2530778344-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKV_20221026T110036_0710_01 | G2530780217-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKU_20221026T110036_0710_01 | G2530780251-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKT_20221026T110036_0710_01 | G2530780282-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGC_20221026T110036_0710_01 | G2530780296-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGE_20221030T092522_0710_01 | G2535607120-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGD_20221030T092522_0710_01 | G2535607552-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGC_20221030T092522_0710_01 | G2535607555-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKT_20221030T092522_0710_01 | G2535609921-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01 | G2535610056-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKV_20221030T092522_0710_01 | G2535611479-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24494_003_11SKT_20221031T083716_0710_01 | G2536450014-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGC_20221101T155724_0710_01 | G2539193798-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGD_20221101T155724_0710_01 | G2539195601-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGE_20221101T155724_0710_01 | G2539196967-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_11SKV_20221101T155724_0710_01 | G2539199950-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKT_20221101T155816_0710_01 | G2539203544-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKV_20221101T155816_0710_01 | G2539203547-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGE_20221101T155816_0710_01 | G2539203627-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGD_20221101T155816_0710_01 | G2539205423-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGC_20221101T155816_0710_01 | G2539205436-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKU_20221101T155816_0710_01 | G2539206874-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGE_20221103T074954_0710_01 | G2541389219-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGD_20221103T074954_0710_01 | G2541389633-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGC_20221103T074954_0710_01 | G2541390013-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKV_20221103T074954_0710_01 | G2541390707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKT_20221103T074954_0710_01 | G2541390948-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKU_20221103T074954_0710_01 | G2541391017-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_006_11SKV_20221103T075046_0710_01 | G2541390173-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGC_20221104T151000_0710_01 | G2543733039-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGD_20221104T151000_0710_01 | G2543733267-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGD_20221104T151052_0710_01 | G2543738483-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_11SKT_20221104T151052_0710_01 | G2543739516-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGC_20221104T151052_0710_01 | G2543740062-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGD_20221105T142134_0710_01 | G2545346904-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGE_20221105T142134_0710_01 | G2545347453-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_11SKV_20221105T142134_0710_01 | G2545348685-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGC_20221105T142226_0710_01 | G2545349703-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGD_20221105T142226_0710_01 | G2545349707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGE_20221105T142226_0710_01 | G2545350290-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKV_20221105T142226_0710_01 | G2545350562-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKT_20221105T142226_0710_01 | G2545350572-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKU_20221105T142226_0710_01 | G2545351427-LPCLOUD\n\n\n\npprint(granules[0])\n\n{'boxes': ['33.309906 -120.259598 34.3242 -119.044289'],\n 'browse_flag': True,\n 'collection_concept_id': 'C2076090826-LPCLOUD',\n 'coordinate_system': 'GEODETIC',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'day_night_flag': 'NIGHT',\n 'granule_size': '3.36234',\n 'id': 'G2530780237-LPCLOUD',\n 'links': [{'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json '\n                     '(VIEW RELATED INFORMATION)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png'},\n           {'href': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_calculated_spatial_domains': [{'start_orbit_number': '24418',\n                                       'stop_orbit_number': '24418'}],\n 'original_format': 'ECHO10',\n 'producer_granule_id': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'time_end': '2022-10-26T11:00:36.970Z',\n 'time_start': '2022-10-26T10:59:45.000Z',\n 'title': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'updated': '2022-10-28T10:41:15.849Z'}"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#get-urls-to-cloud-data-assets",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#get-urls-to-cloud-data-assets",
    "title": "Data discovery with NASA’s CMR",
    "section": "Get URLs to cloud data assets",
    "text": "Get URLs to cloud data assets\n\nhttps_urls = [l['href'] for l in granules[13]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']\n\n\n\ns3_urls = [l['href'] for l in granules[13]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls\n\n['s3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "",
    "text": "import requests\nimport netrc\nfrom datetime import datetime\nimport json\nimport os"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Read netrc file to get EDL information",
    "text": "Read netrc file to get EDL information\n\ndef get_edl_creds():\n    nc = netrc.netrc()\n    remoteHostName = \"urs.earthdata.nasa.gov\"\n    edl_creds = nc.authenticators(remoteHostName)\n    return {'username':edl_creds[0], 'password':edl_creds[2]}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens",
    "text": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens\n\nedl_token_urls = {\n    'generate_token':'https://urs.earthdata.nasa.gov/api/users/token',\n    'list_token':'https://urs.earthdata.nasa.gov/api/users/tokens',\n    'revoke_token': 'https://urs.earthdata.nasa.gov/api/users/revoke_token'\n}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Create a hidden directory to store the output json file with EDL tokens",
    "text": "Create a hidden directory to store the output json file with EDL tokens\n\nif not os.path.isdir('../../../.hidden_dir'):\n    os.mkdir('../../../.hidden_dir')"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Check if a valid token exists or generate a new token",
    "text": "Check if a valid token exists or generate a new token\n\nif len(list_tokens := requests.get(edl_token_urls['list_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password'])).json()) &lt; 1:\n    #print('No tokens available. Generating new Earthdata Login Token ...')\n    generate_token_url = \"https://urs.earthdata.nasa.gov/api/users/token\"\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelif datetime.strptime(list_tokens[0]['expiration_date'], \"%m/%d/%Y\") &lt; datetime.now():\n    #print('Available token is expired. Generating a new Earthdata Login Token ...')\n    revoke_token = requests.post(f\"{edl_token_urls['revoke_token']}?token={list_tokens[0]}\", auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelse:\n    #print('Earthdata Login Token Found ...')\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(list_tokens[0], outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\n\nYour EDL token information can be found here: /home/jovyan/.hidden_dir/edl_token.json"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Resources",
    "text": "Resources\n\nhttps://wiki.earthdata.nasa.gov/display/EL/How+to+Generate+a+User+Token\nhttps://urs.earthdata.nasa.gov/documentation/for_users/user_token"
  }
]